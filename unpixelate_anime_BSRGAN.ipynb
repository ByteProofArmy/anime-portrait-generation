{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses gpu if one exists otherwise just cpu\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_FOLDER = '/Users/fizzausman/Desktop/BSRGAN-finnnn/results' #training images stored here\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "DIFFUSION_STEPS = 1000\n",
    "\n",
    "EPOCHS = 100\n",
    "MAX_BATCHES = 200           \n",
    "LR = 2e-4\n",
    "\n",
    "# what it creates\n",
    "SAMPLE_BATCH = 8\n",
    "OUT_DIR = './samples'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "#reads images and uses magic methods in python\n",
    "class ReadingImages(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.files = sorted(glob.glob(os.path.join(folder, \"*.png\")) + glob.glob(os.path.join(folder, \"*.jpg\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "#we have added random flipping, color jittering and all to benefit training. \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(), #hair must not always be on left side\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.08),                         \n",
    "    transforms.ToTensor(),      \n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),  \n",
    "])\n",
    "\n",
    "dataset = ReadingImages(DATA_FOLDER, transform)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffusion factors\n",
    "betas = torch.linspace(1e-4, 0.02, DIFFUSION_STEPS, device=DEVICE)  # amount of noise added at this step\n",
    "alphas = 1.0 - betas #how much of image remains\n",
    "cum_alpha_bar = torch.cumprod(alphas, dim=0)                           # cumulative image left after many steps\n",
    "alpha_bar_prev = torch.cat([torch.tensor([1.0], device=DEVICE), cum_alpha_bar[:-1]])\n",
    "\n",
    "#time embedding\n",
    "class SinCosPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "\n",
    "    def making_embedding(self, t):\n",
    "        half = self.dim // 2\n",
    "        t = t.float() / float(DIFFUSION_STEPS)         # scale time to [0,1] \n",
    "        freqs = torch.exp(-math.log(20000) * torch.arange(half, device=t.device).float() / half)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            embedding = torch.cat([embedding, torch.zeros(t.size(0), 1, device=t.device)], dim=-1)\n",
    "\n",
    "        embedding = self.post_mlp(embedding)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self,t):\n",
    "        return self.making_embedding(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refines image features \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8 if out_ch >= 8 else 1, out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8 if out_ch >= 8 else 1, out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.time_proj = nn.Linear(time_emb_dim, out_ch) if time_emb_dim else None\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h = self.path1(x)\n",
    "\n",
    "        if t_emb is not None and self.time_proj is not None:\n",
    "            h = h + self.time_proj(t_emb)[:,:,None,None]\n",
    "\n",
    "        h= self.path2(h)\n",
    "\n",
    "        return h + self.skip(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on this device cpu\n",
      "Epoch 000 Batch 0000  Loss: 1.205566\n",
      "Epoch 000 Batch 0010  Loss: 0.554684\n",
      "Epoch 000 Batch 0020  Loss: 0.209574\n",
      "Epoch 000 Batch 0030  Loss: 0.160273\n",
      "Epoch 000 Batch 0040  Loss: 0.191099\n",
      "Epoch 000 Batch 0050  Loss: 0.123958\n",
      "Epoch 000 Batch 0060  Loss: 0.120723\n",
      "Epoch 000 Batch 0070  Loss: 0.138267\n",
      "Epoch 000 Batch 0080  Loss: 0.075937\n",
      "Epoch 000 Batch 0090  Loss: 0.084412\n",
      "Epoch 000 Batch 0100  Loss: 0.107960\n",
      "Epoch 000 Batch 0110  Loss: 0.096695\n",
      "Epoch 000 Batch 0120  Loss: 0.090893\n",
      "Epoch 000 Batch 0130  Loss: 0.143969\n",
      "Epoch 000 Batch 0140  Loss: 0.100229\n",
      "Epoch 000 Batch 0150  Loss: 0.127983\n",
      "Epoch 000 Batch 0160  Loss: 0.063832\n",
      "Epoch 000 Batch 0170  Loss: 0.121347\n",
      "Epoch 000 Batch 0180  Loss: 0.115272\n",
      "Epoch 000 Batch 0190  Loss: 0.175867\n",
      "Saved sample grid to ./samples/sample_epoch_000.png\n",
      "Epoch 001 Batch 0000  Loss: 0.055919\n",
      "Epoch 001 Batch 0010  Loss: 0.139013\n",
      "Epoch 001 Batch 0020  Loss: 0.085872\n",
      "Epoch 001 Batch 0030  Loss: 0.088676\n",
      "Epoch 001 Batch 0040  Loss: 0.057165\n",
      "Epoch 001 Batch 0050  Loss: 0.103377\n",
      "Epoch 001 Batch 0060  Loss: 0.073088\n",
      "Epoch 001 Batch 0070  Loss: 0.041336\n",
      "Epoch 001 Batch 0080  Loss: 0.043316\n",
      "Epoch 001 Batch 0090  Loss: 0.044327\n",
      "Epoch 001 Batch 0100  Loss: 0.031513\n",
      "Epoch 001 Batch 0110  Loss: 0.057007\n",
      "Epoch 001 Batch 0120  Loss: 0.055034\n",
      "Epoch 001 Batch 0130  Loss: 0.042127\n",
      "Epoch 001 Batch 0140  Loss: 0.097510\n",
      "Epoch 001 Batch 0150  Loss: 0.032302\n",
      "Epoch 001 Batch 0160  Loss: 0.036892\n",
      "Epoch 001 Batch 0170  Loss: 0.056565\n",
      "Epoch 001 Batch 0180  Loss: 0.039935\n",
      "Epoch 001 Batch 0190  Loss: 0.089117\n",
      "Saved sample grid to ./samples/sample_epoch_001.png\n",
      "Epoch 002 Batch 0000  Loss: 0.082713\n",
      "Epoch 002 Batch 0010  Loss: 0.110665\n",
      "Epoch 002 Batch 0020  Loss: 0.041974\n",
      "Epoch 002 Batch 0030  Loss: 0.087154\n",
      "Epoch 002 Batch 0040  Loss: 0.072416\n",
      "Epoch 002 Batch 0050  Loss: 0.088566\n",
      "Epoch 002 Batch 0060  Loss: 0.018308\n",
      "Epoch 002 Batch 0070  Loss: 0.027201\n",
      "Epoch 002 Batch 0080  Loss: 0.076743\n",
      "Epoch 002 Batch 0090  Loss: 0.025583\n",
      "Epoch 002 Batch 0100  Loss: 0.040720\n",
      "Epoch 002 Batch 0110  Loss: 0.046271\n",
      "Epoch 002 Batch 0120  Loss: 0.049143\n",
      "Epoch 002 Batch 0130  Loss: 0.058932\n",
      "Epoch 002 Batch 0140  Loss: 0.016599\n",
      "Epoch 002 Batch 0150  Loss: 0.033768\n",
      "Epoch 002 Batch 0160  Loss: 0.056856\n",
      "Epoch 002 Batch 0170  Loss: 0.118383\n",
      "Epoch 002 Batch 0180  Loss: 0.029136\n",
      "Epoch 002 Batch 0190  Loss: 0.050349\n",
      "Saved sample grid to ./samples/sample_epoch_002.png\n",
      "Epoch 003 Batch 0000  Loss: 0.029433\n",
      "Epoch 003 Batch 0010  Loss: 0.071185\n",
      "Epoch 003 Batch 0020  Loss: 0.026283\n",
      "Epoch 003 Batch 0030  Loss: 0.035623\n",
      "Epoch 003 Batch 0040  Loss: 0.029708\n",
      "Epoch 003 Batch 0050  Loss: 0.079817\n",
      "Epoch 003 Batch 0060  Loss: 0.015229\n",
      "Epoch 003 Batch 0070  Loss: 0.033260\n",
      "Epoch 003 Batch 0080  Loss: 0.020433\n",
      "Epoch 003 Batch 0090  Loss: 0.068200\n",
      "Epoch 003 Batch 0100  Loss: 0.036853\n",
      "Epoch 003 Batch 0110  Loss: 0.051068\n",
      "Epoch 003 Batch 0120  Loss: 0.018415\n",
      "Epoch 003 Batch 0130  Loss: 0.039923\n",
      "Epoch 003 Batch 0140  Loss: 0.018013\n",
      "Epoch 003 Batch 0150  Loss: 0.084441\n",
      "Epoch 003 Batch 0160  Loss: 0.089831\n",
      "Epoch 003 Batch 0170  Loss: 0.077095\n",
      "Epoch 003 Batch 0180  Loss: 0.102457\n",
      "Epoch 003 Batch 0190  Loss: 0.038621\n",
      "Saved sample grid to ./samples/sample_epoch_003.png\n",
      "Epoch 004 Batch 0000  Loss: 0.028053\n",
      "Epoch 004 Batch 0010  Loss: 0.079798\n",
      "Epoch 004 Batch 0020  Loss: 0.087337\n",
      "Epoch 004 Batch 0030  Loss: 0.030131\n",
      "Epoch 004 Batch 0040  Loss: 0.022438\n",
      "Epoch 004 Batch 0050  Loss: 0.055205\n",
      "Epoch 004 Batch 0060  Loss: 0.032169\n",
      "Epoch 004 Batch 0070  Loss: 0.031831\n",
      "Epoch 004 Batch 0080  Loss: 0.098890\n",
      "Epoch 004 Batch 0090  Loss: 0.041111\n",
      "Epoch 004 Batch 0100  Loss: 0.029993\n",
      "Epoch 004 Batch 0110  Loss: 0.064141\n",
      "Epoch 004 Batch 0120  Loss: 0.050139\n",
      "Epoch 004 Batch 0130  Loss: 0.045600\n",
      "Epoch 004 Batch 0140  Loss: 0.029502\n",
      "Epoch 004 Batch 0150  Loss: 0.039760\n",
      "Epoch 004 Batch 0160  Loss: 0.024205\n",
      "Epoch 004 Batch 0170  Loss: 0.021116\n",
      "Epoch 004 Batch 0180  Loss: 0.020300\n",
      "Epoch 004 Batch 0190  Loss: 0.057516\n",
      "Saved sample grid to ./samples/sample_epoch_004.png\n",
      "Epoch 005 Batch 0000  Loss: 0.056167\n",
      "Epoch 005 Batch 0010  Loss: 0.028004\n",
      "Epoch 005 Batch 0020  Loss: 0.083201\n",
      "Epoch 005 Batch 0030  Loss: 0.069719\n",
      "Epoch 005 Batch 0040  Loss: 0.063738\n",
      "Epoch 005 Batch 0050  Loss: 0.044195\n",
      "Epoch 005 Batch 0060  Loss: 0.028237\n",
      "Epoch 005 Batch 0070  Loss: 0.064300\n",
      "Epoch 005 Batch 0080  Loss: 0.058329\n",
      "Epoch 005 Batch 0090  Loss: 0.030681\n",
      "Epoch 005 Batch 0100  Loss: 0.064850\n",
      "Epoch 005 Batch 0110  Loss: 0.046021\n",
      "Epoch 005 Batch 0120  Loss: 0.049181\n",
      "Epoch 005 Batch 0130  Loss: 0.055928\n",
      "Epoch 005 Batch 0140  Loss: 0.065297\n",
      "Epoch 005 Batch 0150  Loss: 0.037265\n",
      "Epoch 005 Batch 0160  Loss: 0.055567\n",
      "Epoch 005 Batch 0170  Loss: 0.020391\n",
      "Epoch 005 Batch 0180  Loss: 0.016656\n",
      "Epoch 005 Batch 0190  Loss: 0.038492\n",
      "Saved sample grid to ./samples/sample_epoch_005.png\n",
      "Epoch 006 Batch 0000  Loss: 0.025426\n",
      "Epoch 006 Batch 0010  Loss: 0.079404\n",
      "Epoch 006 Batch 0020  Loss: 0.037228\n",
      "Epoch 006 Batch 0030  Loss: 0.034809\n",
      "Epoch 006 Batch 0040  Loss: 0.031315\n",
      "Epoch 006 Batch 0050  Loss: 0.039122\n",
      "Epoch 006 Batch 0060  Loss: 0.024318\n",
      "Epoch 006 Batch 0070  Loss: 0.052167\n",
      "Epoch 006 Batch 0080  Loss: 0.016130\n",
      "Epoch 006 Batch 0090  Loss: 0.095181\n",
      "Epoch 006 Batch 0100  Loss: 0.020874\n",
      "Epoch 006 Batch 0110  Loss: 0.032505\n",
      "Epoch 006 Batch 0120  Loss: 0.024004\n",
      "Epoch 006 Batch 0130  Loss: 0.035733\n",
      "Epoch 006 Batch 0140  Loss: 0.030178\n",
      "Epoch 006 Batch 0150  Loss: 0.033641\n",
      "Epoch 006 Batch 0160  Loss: 0.042667\n",
      "Epoch 006 Batch 0170  Loss: 0.040969\n",
      "Epoch 006 Batch 0180  Loss: 0.138406\n",
      "Epoch 006 Batch 0190  Loss: 0.037493\n",
      "Saved sample grid to ./samples/sample_epoch_006.png\n",
      "Epoch 007 Batch 0000  Loss: 0.034267\n",
      "Epoch 007 Batch 0010  Loss: 0.038282\n",
      "Epoch 007 Batch 0020  Loss: 0.035503\n",
      "Epoch 007 Batch 0030  Loss: 0.047563\n",
      "Epoch 007 Batch 0040  Loss: 0.024569\n",
      "Epoch 007 Batch 0050  Loss: 0.026860\n",
      "Epoch 007 Batch 0060  Loss: 0.034194\n",
      "Epoch 007 Batch 0070  Loss: 0.052426\n",
      "Epoch 007 Batch 0080  Loss: 0.051952\n",
      "Epoch 007 Batch 0090  Loss: 0.030822\n",
      "Epoch 007 Batch 0100  Loss: 0.030391\n",
      "Epoch 007 Batch 0110  Loss: 0.027451\n",
      "Epoch 007 Batch 0120  Loss: 0.023089\n",
      "Epoch 007 Batch 0130  Loss: 0.027114\n",
      "Epoch 007 Batch 0140  Loss: 0.031633\n",
      "Epoch 007 Batch 0150  Loss: 0.023937\n",
      "Epoch 007 Batch 0160  Loss: 0.082998\n",
      "Epoch 007 Batch 0170  Loss: 0.064112\n",
      "Epoch 007 Batch 0180  Loss: 0.029063\n",
      "Epoch 007 Batch 0190  Loss: 0.025880\n",
      "Saved sample grid to ./samples/sample_epoch_007.png\n",
      "Epoch 008 Batch 0000  Loss: 0.051320\n",
      "Epoch 008 Batch 0010  Loss: 0.008658\n",
      "Epoch 008 Batch 0020  Loss: 0.035970\n",
      "Epoch 008 Batch 0030  Loss: 0.042939\n",
      "Epoch 008 Batch 0040  Loss: 0.062072\n",
      "Epoch 008 Batch 0050  Loss: 0.011741\n",
      "Epoch 008 Batch 0060  Loss: 0.014518\n",
      "Epoch 008 Batch 0070  Loss: 0.034921\n",
      "Epoch 008 Batch 0080  Loss: 0.025046\n",
      "Epoch 008 Batch 0090  Loss: 0.061163\n",
      "Epoch 008 Batch 0100  Loss: 0.024179\n",
      "Epoch 008 Batch 0110  Loss: 0.026711\n",
      "Epoch 008 Batch 0120  Loss: 0.038049\n",
      "Epoch 008 Batch 0130  Loss: 0.032792\n",
      "Epoch 008 Batch 0140  Loss: 0.044309\n",
      "Epoch 008 Batch 0150  Loss: 0.056545\n",
      "Epoch 008 Batch 0160  Loss: 0.023693\n",
      "Epoch 008 Batch 0170  Loss: 0.050385\n",
      "Epoch 008 Batch 0180  Loss: 0.037616\n",
      "Epoch 008 Batch 0190  Loss: 0.041508\n",
      "Saved sample grid to ./samples/sample_epoch_008.png\n",
      "Epoch 009 Batch 0000  Loss: 0.027608\n",
      "Epoch 009 Batch 0010  Loss: 0.052982\n",
      "Epoch 009 Batch 0020  Loss: 0.056496\n",
      "Epoch 009 Batch 0030  Loss: 0.036868\n",
      "Epoch 009 Batch 0040  Loss: 0.015792\n",
      "Epoch 009 Batch 0050  Loss: 0.105192\n",
      "Epoch 009 Batch 0060  Loss: 0.019147\n",
      "Epoch 009 Batch 0070  Loss: 0.018454\n",
      "Epoch 009 Batch 0080  Loss: 0.027158\n",
      "Epoch 009 Batch 0090  Loss: 0.035812\n",
      "Epoch 009 Batch 0100  Loss: 0.072696\n",
      "Epoch 009 Batch 0110  Loss: 0.047546\n",
      "Epoch 009 Batch 0120  Loss: 0.066379\n",
      "Epoch 009 Batch 0130  Loss: 0.027656\n",
      "Epoch 009 Batch 0140  Loss: 0.055731\n",
      "Epoch 009 Batch 0150  Loss: 0.028405\n",
      "Epoch 009 Batch 0160  Loss: 0.124417\n",
      "Epoch 009 Batch 0170  Loss: 0.037831\n",
      "Epoch 009 Batch 0180  Loss: 0.058835\n",
      "Epoch 009 Batch 0190  Loss: 0.041102\n",
      "Saved sample grid to ./samples/sample_epoch_009.png\n",
      "Epoch 010 Batch 0000  Loss: 0.024973\n",
      "Epoch 010 Batch 0010  Loss: 0.025660\n",
      "Epoch 010 Batch 0020  Loss: 0.044049\n",
      "Epoch 010 Batch 0030  Loss: 0.054734\n",
      "Epoch 010 Batch 0040  Loss: 0.018036\n",
      "Epoch 010 Batch 0050  Loss: 0.026219\n",
      "Epoch 010 Batch 0060  Loss: 0.031243\n",
      "Epoch 010 Batch 0070  Loss: 0.063843\n",
      "Epoch 010 Batch 0080  Loss: 0.049667\n",
      "Epoch 010 Batch 0090  Loss: 0.027825\n",
      "Epoch 010 Batch 0100  Loss: 0.027895\n",
      "Epoch 010 Batch 0110  Loss: 0.014285\n",
      "Epoch 010 Batch 0120  Loss: 0.021008\n",
      "Epoch 010 Batch 0130  Loss: 0.027852\n",
      "Epoch 010 Batch 0140  Loss: 0.060339\n",
      "Epoch 010 Batch 0150  Loss: 0.021445\n",
      "Epoch 010 Batch 0160  Loss: 0.026148\n",
      "Epoch 010 Batch 0170  Loss: 0.033764\n",
      "Epoch 010 Batch 0180  Loss: 0.018508\n",
      "Epoch 010 Batch 0190  Loss: 0.023332\n",
      "Saved sample grid to ./samples/sample_epoch_010.png\n",
      "Epoch 011 Batch 0000  Loss: 0.015284\n",
      "Epoch 011 Batch 0010  Loss: 0.007936\n",
      "Epoch 011 Batch 0020  Loss: 0.030868\n",
      "Epoch 011 Batch 0030  Loss: 0.049497\n",
      "Epoch 011 Batch 0040  Loss: 0.037158\n",
      "Epoch 011 Batch 0050  Loss: 0.058244\n",
      "Epoch 011 Batch 0060  Loss: 0.056123\n",
      "Epoch 011 Batch 0070  Loss: 0.023234\n",
      "Epoch 011 Batch 0080  Loss: 0.028131\n",
      "Epoch 011 Batch 0090  Loss: 0.022641\n",
      "Epoch 011 Batch 0100  Loss: 0.127054\n",
      "Epoch 011 Batch 0110  Loss: 0.052135\n",
      "Epoch 011 Batch 0120  Loss: 0.060673\n",
      "Epoch 011 Batch 0130  Loss: 0.038105\n",
      "Epoch 011 Batch 0140  Loss: 0.027080\n",
      "Epoch 011 Batch 0150  Loss: 0.030889\n",
      "Epoch 011 Batch 0160  Loss: 0.030253\n",
      "Epoch 011 Batch 0170  Loss: 0.076159\n",
      "Epoch 011 Batch 0180  Loss: 0.026407\n",
      "Epoch 011 Batch 0190  Loss: 0.034531\n",
      "Saved sample grid to ./samples/sample_epoch_011.png\n",
      "Epoch 012 Batch 0000  Loss: 0.022395\n",
      "Epoch 012 Batch 0010  Loss: 0.030013\n",
      "Epoch 012 Batch 0020  Loss: 0.036707\n",
      "Epoch 012 Batch 0030  Loss: 0.045955\n",
      "Epoch 012 Batch 0040  Loss: 0.042219\n",
      "Epoch 012 Batch 0050  Loss: 0.008694\n",
      "Epoch 012 Batch 0060  Loss: 0.064885\n",
      "Epoch 012 Batch 0070  Loss: 0.012035\n",
      "Epoch 012 Batch 0080  Loss: 0.030124\n",
      "Epoch 012 Batch 0090  Loss: 0.032480\n",
      "Epoch 012 Batch 0100  Loss: 0.029377\n",
      "Epoch 012 Batch 0110  Loss: 0.056142\n",
      "Epoch 012 Batch 0120  Loss: 0.075672\n",
      "Epoch 012 Batch 0130  Loss: 0.020007\n",
      "Epoch 012 Batch 0140  Loss: 0.057832\n",
      "Epoch 012 Batch 0150  Loss: 0.054474\n",
      "Epoch 012 Batch 0160  Loss: 0.044987\n",
      "Epoch 012 Batch 0170  Loss: 0.049381\n",
      "Epoch 012 Batch 0180  Loss: 0.036327\n",
      "Epoch 012 Batch 0190  Loss: 0.030360\n",
      "Saved sample grid to ./samples/sample_epoch_012.png\n",
      "Epoch 013 Batch 0000  Loss: 0.051142\n",
      "Epoch 013 Batch 0010  Loss: 0.027773\n",
      "Epoch 013 Batch 0020  Loss: 0.055657\n",
      "Epoch 013 Batch 0030  Loss: 0.022037\n",
      "Epoch 013 Batch 0040  Loss: 0.021873\n",
      "Epoch 013 Batch 0050  Loss: 0.016146\n",
      "Epoch 013 Batch 0060  Loss: 0.078701\n",
      "Epoch 013 Batch 0070  Loss: 0.053251\n",
      "Epoch 013 Batch 0080  Loss: 0.055660\n",
      "Epoch 013 Batch 0090  Loss: 0.044718\n",
      "Epoch 013 Batch 0100  Loss: 0.043881\n",
      "Epoch 013 Batch 0110  Loss: 0.023084\n",
      "Epoch 013 Batch 0120  Loss: 0.033416\n",
      "Epoch 013 Batch 0130  Loss: 0.028119\n",
      "Epoch 013 Batch 0140  Loss: 0.011224\n",
      "Epoch 013 Batch 0150  Loss: 0.017817\n",
      "Epoch 013 Batch 0160  Loss: 0.012860\n",
      "Epoch 013 Batch 0170  Loss: 0.055882\n",
      "Epoch 013 Batch 0180  Loss: 0.019706\n",
      "Epoch 013 Batch 0190  Loss: 0.025040\n",
      "Saved sample grid to ./samples/sample_epoch_013.png\n",
      "Epoch 014 Batch 0000  Loss: 0.024547\n",
      "Epoch 014 Batch 0010  Loss: 0.055989\n",
      "Epoch 014 Batch 0020  Loss: 0.044602\n",
      "Epoch 014 Batch 0030  Loss: 0.083926\n",
      "Epoch 014 Batch 0040  Loss: 0.018906\n",
      "Epoch 014 Batch 0050  Loss: 0.017980\n",
      "Epoch 014 Batch 0060  Loss: 0.008221\n",
      "Epoch 014 Batch 0070  Loss: 0.023976\n",
      "Epoch 014 Batch 0080  Loss: 0.048449\n",
      "Epoch 014 Batch 0090  Loss: 0.026488\n",
      "Epoch 014 Batch 0100  Loss: 0.019759\n",
      "Epoch 014 Batch 0110  Loss: 0.027159\n",
      "Epoch 014 Batch 0120  Loss: 0.028082\n",
      "Epoch 014 Batch 0130  Loss: 0.050577\n",
      "Epoch 014 Batch 0140  Loss: 0.037894\n",
      "Epoch 014 Batch 0150  Loss: 0.045437\n",
      "Epoch 014 Batch 0160  Loss: 0.025208\n",
      "Epoch 014 Batch 0170  Loss: 0.033114\n",
      "Epoch 014 Batch 0180  Loss: 0.051591\n",
      "Epoch 014 Batch 0190  Loss: 0.029712\n",
      "Saved sample grid to ./samples/sample_epoch_014.png\n",
      "Epoch 015 Batch 0000  Loss: 0.027808\n",
      "Epoch 015 Batch 0010  Loss: 0.031624\n",
      "Epoch 015 Batch 0020  Loss: 0.017721\n",
      "Epoch 015 Batch 0030  Loss: 0.020570\n",
      "Epoch 015 Batch 0040  Loss: 0.021204\n",
      "Epoch 015 Batch 0050  Loss: 0.039615\n",
      "Epoch 015 Batch 0060  Loss: 0.037255\n",
      "Epoch 015 Batch 0070  Loss: 0.051527\n",
      "Epoch 015 Batch 0080  Loss: 0.038946\n",
      "Epoch 015 Batch 0090  Loss: 0.017182\n",
      "Epoch 015 Batch 0100  Loss: 0.045151\n",
      "Epoch 015 Batch 0110  Loss: 0.024937\n",
      "Epoch 015 Batch 0120  Loss: 0.051122\n",
      "Epoch 015 Batch 0130  Loss: 0.033204\n",
      "Epoch 015 Batch 0140  Loss: 0.017300\n",
      "Epoch 015 Batch 0150  Loss: 0.030679\n",
      "Epoch 015 Batch 0160  Loss: 0.048078\n",
      "Epoch 015 Batch 0170  Loss: 0.034016\n",
      "Epoch 015 Batch 0180  Loss: 0.050387\n",
      "Epoch 015 Batch 0190  Loss: 0.054104\n",
      "Saved sample grid to ./samples/sample_epoch_015.png\n",
      "Epoch 016 Batch 0000  Loss: 0.050681\n",
      "Epoch 016 Batch 0010  Loss: 0.018523\n",
      "Epoch 016 Batch 0020  Loss: 0.020237\n",
      "Epoch 016 Batch 0030  Loss: 0.031369\n",
      "Epoch 016 Batch 0040  Loss: 0.014863\n",
      "Epoch 016 Batch 0050  Loss: 0.023706\n",
      "Epoch 016 Batch 0060  Loss: 0.020840\n",
      "Epoch 016 Batch 0070  Loss: 0.078683\n",
      "Epoch 016 Batch 0080  Loss: 0.055101\n",
      "Epoch 016 Batch 0090  Loss: 0.047511\n",
      "Epoch 016 Batch 0100  Loss: 0.024582\n",
      "Epoch 016 Batch 0110  Loss: 0.019606\n",
      "Epoch 016 Batch 0120  Loss: 0.102703\n",
      "Epoch 016 Batch 0130  Loss: 0.047735\n",
      "Epoch 016 Batch 0140  Loss: 0.052142\n",
      "Epoch 016 Batch 0150  Loss: 0.026304\n",
      "Epoch 016 Batch 0160  Loss: 0.039822\n",
      "Epoch 016 Batch 0170  Loss: 0.071008\n",
      "Epoch 016 Batch 0180  Loss: 0.035469\n",
      "Epoch 016 Batch 0190  Loss: 0.049788\n",
      "Saved sample grid to ./samples/sample_epoch_016.png\n",
      "Epoch 017 Batch 0000  Loss: 0.027620\n",
      "Epoch 017 Batch 0010  Loss: 0.050212\n",
      "Epoch 017 Batch 0020  Loss: 0.038152\n",
      "Epoch 017 Batch 0030  Loss: 0.037735\n",
      "Epoch 017 Batch 0040  Loss: 0.018500\n",
      "Epoch 017 Batch 0050  Loss: 0.035595\n",
      "Epoch 017 Batch 0060  Loss: 0.029520\n",
      "Epoch 017 Batch 0070  Loss: 0.052351\n",
      "Epoch 017 Batch 0080  Loss: 0.017052\n",
      "Epoch 017 Batch 0090  Loss: 0.029419\n",
      "Epoch 017 Batch 0100  Loss: 0.032446\n",
      "Epoch 017 Batch 0110  Loss: 0.029287\n",
      "Epoch 017 Batch 0120  Loss: 0.053215\n",
      "Epoch 017 Batch 0130  Loss: 0.028185\n",
      "Epoch 017 Batch 0140  Loss: 0.036930\n",
      "Epoch 017 Batch 0150  Loss: 0.052956\n",
      "Epoch 017 Batch 0160  Loss: 0.054349\n",
      "Epoch 017 Batch 0170  Loss: 0.050501\n",
      "Epoch 017 Batch 0180  Loss: 0.065898\n",
      "Epoch 017 Batch 0190  Loss: 0.035302\n",
      "Saved sample grid to ./samples/sample_epoch_017.png\n",
      "Epoch 018 Batch 0000  Loss: 0.015155\n",
      "Epoch 018 Batch 0010  Loss: 0.030340\n",
      "Epoch 018 Batch 0020  Loss: 0.026311\n",
      "Epoch 018 Batch 0030  Loss: 0.030518\n",
      "Epoch 018 Batch 0040  Loss: 0.028917\n",
      "Epoch 018 Batch 0050  Loss: 0.038675\n",
      "Epoch 018 Batch 0060  Loss: 0.015540\n",
      "Epoch 018 Batch 0070  Loss: 0.017935\n",
      "Epoch 018 Batch 0080  Loss: 0.044397\n",
      "Epoch 018 Batch 0090  Loss: 0.035997\n",
      "Epoch 018 Batch 0100  Loss: 0.020345\n",
      "Epoch 018 Batch 0110  Loss: 0.016844\n",
      "Epoch 018 Batch 0120  Loss: 0.036935\n",
      "Epoch 018 Batch 0130  Loss: 0.015188\n",
      "Epoch 018 Batch 0140  Loss: 0.013862\n",
      "Epoch 018 Batch 0150  Loss: 0.025056\n",
      "Epoch 018 Batch 0160  Loss: 0.065689\n",
      "Epoch 018 Batch 0170  Loss: 0.026941\n",
      "Epoch 018 Batch 0180  Loss: 0.026813\n",
      "Epoch 018 Batch 0190  Loss: 0.015786\n",
      "Saved sample grid to ./samples/sample_epoch_018.png\n",
      "Epoch 019 Batch 0000  Loss: 0.079156\n",
      "Epoch 019 Batch 0010  Loss: 0.022572\n",
      "Epoch 019 Batch 0020  Loss: 0.048362\n",
      "Epoch 019 Batch 0030  Loss: 0.015859\n",
      "Epoch 019 Batch 0040  Loss: 0.046020\n",
      "Epoch 019 Batch 0050  Loss: 0.010515\n",
      "Epoch 019 Batch 0060  Loss: 0.035616\n",
      "Epoch 019 Batch 0070  Loss: 0.022162\n",
      "Epoch 019 Batch 0080  Loss: 0.013240\n",
      "Epoch 019 Batch 0090  Loss: 0.018034\n",
      "Epoch 019 Batch 0100  Loss: 0.049142\n",
      "Epoch 019 Batch 0110  Loss: 0.038711\n",
      "Epoch 019 Batch 0120  Loss: 0.025143\n",
      "Epoch 019 Batch 0130  Loss: 0.020309\n",
      "Epoch 019 Batch 0140  Loss: 0.024855\n",
      "Epoch 019 Batch 0150  Loss: 0.016444\n",
      "Epoch 019 Batch 0160  Loss: 0.033845\n",
      "Epoch 019 Batch 0170  Loss: 0.041073\n",
      "Epoch 019 Batch 0180  Loss: 0.032103\n",
      "Epoch 019 Batch 0190  Loss: 0.036557\n",
      "Saved sample grid to ./samples/sample_epoch_019.png\n",
      "Epoch 020 Batch 0000  Loss: 0.098977\n",
      "Epoch 020 Batch 0010  Loss: 0.029746\n",
      "Epoch 020 Batch 0020  Loss: 0.038101\n",
      "Epoch 020 Batch 0030  Loss: 0.044719\n",
      "Epoch 020 Batch 0040  Loss: 0.062108\n",
      "Epoch 020 Batch 0050  Loss: 0.025252\n",
      "Epoch 020 Batch 0060  Loss: 0.080200\n",
      "Epoch 020 Batch 0070  Loss: 0.037920\n",
      "Epoch 020 Batch 0080  Loss: 0.030749\n",
      "Epoch 020 Batch 0090  Loss: 0.029992\n",
      "Epoch 020 Batch 0100  Loss: 0.084519\n",
      "Epoch 020 Batch 0110  Loss: 0.013746\n",
      "Epoch 020 Batch 0120  Loss: 0.040391\n",
      "Epoch 020 Batch 0130  Loss: 0.037374\n",
      "Epoch 020 Batch 0140  Loss: 0.025381\n",
      "Epoch 020 Batch 0150  Loss: 0.019255\n",
      "Epoch 020 Batch 0160  Loss: 0.037728\n",
      "Epoch 020 Batch 0170  Loss: 0.045602\n",
      "Epoch 020 Batch 0180  Loss: 0.013043\n",
      "Epoch 020 Batch 0190  Loss: 0.062705\n",
      "Saved sample grid to ./samples/sample_epoch_020.png\n",
      "Epoch 021 Batch 0000  Loss: 0.053471\n",
      "Epoch 021 Batch 0010  Loss: 0.060227\n",
      "Epoch 021 Batch 0020  Loss: 0.018033\n",
      "Epoch 021 Batch 0030  Loss: 0.049688\n",
      "Epoch 021 Batch 0040  Loss: 0.026742\n",
      "Epoch 021 Batch 0050  Loss: 0.019891\n",
      "Epoch 021 Batch 0060  Loss: 0.009757\n",
      "Epoch 021 Batch 0070  Loss: 0.023955\n",
      "Epoch 021 Batch 0080  Loss: 0.072948\n",
      "Epoch 021 Batch 0090  Loss: 0.019725\n",
      "Epoch 021 Batch 0100  Loss: 0.067063\n",
      "Epoch 021 Batch 0110  Loss: 0.036898\n",
      "Epoch 021 Batch 0120  Loss: 0.034329\n",
      "Epoch 021 Batch 0130  Loss: 0.039487\n",
      "Epoch 021 Batch 0140  Loss: 0.021783\n",
      "Epoch 021 Batch 0150  Loss: 0.059028\n",
      "Epoch 021 Batch 0160  Loss: 0.034631\n",
      "Epoch 021 Batch 0170  Loss: 0.007159\n",
      "Epoch 021 Batch 0180  Loss: 0.037503\n",
      "Epoch 021 Batch 0190  Loss: 0.016908\n",
      "Saved sample grid to ./samples/sample_epoch_021.png\n",
      "Epoch 022 Batch 0000  Loss: 0.039989\n",
      "Epoch 022 Batch 0010  Loss: 0.045156\n",
      "Epoch 022 Batch 0020  Loss: 0.043816\n",
      "Epoch 022 Batch 0030  Loss: 0.070504\n",
      "Epoch 022 Batch 0040  Loss: 0.022003\n",
      "Epoch 022 Batch 0050  Loss: 0.007023\n",
      "Epoch 022 Batch 0060  Loss: 0.053793\n",
      "Epoch 022 Batch 0070  Loss: 0.020197\n",
      "Epoch 022 Batch 0080  Loss: 0.016777\n",
      "Epoch 022 Batch 0090  Loss: 0.046272\n",
      "Epoch 022 Batch 0100  Loss: 0.041887\n",
      "Epoch 022 Batch 0110  Loss: 0.055258\n",
      "Epoch 022 Batch 0120  Loss: 0.029832\n",
      "Epoch 022 Batch 0130  Loss: 0.037145\n",
      "Epoch 022 Batch 0140  Loss: 0.047563\n",
      "Epoch 022 Batch 0150  Loss: 0.079669\n",
      "Epoch 022 Batch 0160  Loss: 0.066312\n",
      "Epoch 022 Batch 0170  Loss: 0.028038\n",
      "Epoch 022 Batch 0180  Loss: 0.033538\n",
      "Epoch 022 Batch 0190  Loss: 0.013989\n",
      "Saved sample grid to ./samples/sample_epoch_022.png\n",
      "Epoch 023 Batch 0000  Loss: 0.014780\n",
      "Epoch 023 Batch 0010  Loss: 0.027361\n",
      "Epoch 023 Batch 0020  Loss: 0.050085\n",
      "Epoch 023 Batch 0030  Loss: 0.020871\n",
      "Epoch 023 Batch 0040  Loss: 0.054834\n",
      "Epoch 023 Batch 0050  Loss: 0.035763\n",
      "Epoch 023 Batch 0060  Loss: 0.027746\n",
      "Epoch 023 Batch 0070  Loss: 0.053354\n",
      "Epoch 023 Batch 0080  Loss: 0.039525\n",
      "Epoch 023 Batch 0090  Loss: 0.044013\n",
      "Epoch 023 Batch 0100  Loss: 0.069287\n",
      "Epoch 023 Batch 0110  Loss: 0.037674\n",
      "Epoch 023 Batch 0120  Loss: 0.018982\n",
      "Epoch 023 Batch 0130  Loss: 0.018329\n",
      "Epoch 023 Batch 0140  Loss: 0.038808\n",
      "Epoch 023 Batch 0150  Loss: 0.025348\n",
      "Epoch 023 Batch 0160  Loss: 0.019621\n",
      "Epoch 023 Batch 0170  Loss: 0.054036\n",
      "Epoch 023 Batch 0180  Loss: 0.033027\n",
      "Epoch 023 Batch 0190  Loss: 0.025026\n",
      "Saved sample grid to ./samples/sample_epoch_023.png\n",
      "Epoch 024 Batch 0000  Loss: 0.008446\n",
      "Epoch 024 Batch 0010  Loss: 0.017369\n",
      "Epoch 024 Batch 0020  Loss: 0.015841\n",
      "Epoch 024 Batch 0030  Loss: 0.021859\n",
      "Epoch 024 Batch 0040  Loss: 0.017731\n",
      "Epoch 024 Batch 0050  Loss: 0.035287\n",
      "Epoch 024 Batch 0060  Loss: 0.030417\n",
      "Epoch 024 Batch 0070  Loss: 0.029421\n",
      "Epoch 024 Batch 0080  Loss: 0.042396\n",
      "Epoch 024 Batch 0090  Loss: 0.011363\n",
      "Epoch 024 Batch 0100  Loss: 0.014581\n",
      "Epoch 024 Batch 0110  Loss: 0.030863\n",
      "Epoch 024 Batch 0120  Loss: 0.069616\n",
      "Epoch 024 Batch 0130  Loss: 0.031372\n",
      "Epoch 024 Batch 0140  Loss: 0.039730\n",
      "Epoch 024 Batch 0150  Loss: 0.035204\n",
      "Epoch 024 Batch 0160  Loss: 0.007783\n",
      "Epoch 024 Batch 0170  Loss: 0.009064\n",
      "Epoch 024 Batch 0180  Loss: 0.021426\n",
      "Epoch 024 Batch 0190  Loss: 0.010729\n",
      "Saved sample grid to ./samples/sample_epoch_024.png\n",
      "Epoch 025 Batch 0000  Loss: 0.028384\n",
      "Epoch 025 Batch 0010  Loss: 0.027454\n",
      "Epoch 025 Batch 0020  Loss: 0.023690\n",
      "Epoch 025 Batch 0030  Loss: 0.036499\n",
      "Epoch 025 Batch 0040  Loss: 0.019684\n",
      "Epoch 025 Batch 0050  Loss: 0.023322\n",
      "Epoch 025 Batch 0060  Loss: 0.022906\n",
      "Epoch 025 Batch 0070  Loss: 0.011159\n",
      "Epoch 025 Batch 0080  Loss: 0.033342\n",
      "Epoch 025 Batch 0090  Loss: 0.034324\n",
      "Epoch 025 Batch 0100  Loss: 0.037482\n",
      "Epoch 025 Batch 0110  Loss: 0.050286\n",
      "Epoch 025 Batch 0120  Loss: 0.031439\n",
      "Epoch 025 Batch 0130  Loss: 0.032315\n",
      "Epoch 025 Batch 0140  Loss: 0.066113\n",
      "Epoch 025 Batch 0150  Loss: 0.023942\n",
      "Epoch 025 Batch 0160  Loss: 0.033824\n",
      "Epoch 025 Batch 0170  Loss: 0.054555\n",
      "Epoch 025 Batch 0180  Loss: 0.021571\n",
      "Epoch 025 Batch 0190  Loss: 0.026390\n",
      "Saved sample grid to ./samples/sample_epoch_025.png\n",
      "Epoch 026 Batch 0000  Loss: 0.020654\n",
      "Epoch 026 Batch 0010  Loss: 0.046569\n",
      "Epoch 026 Batch 0020  Loss: 0.058336\n",
      "Epoch 026 Batch 0030  Loss: 0.052577\n",
      "Epoch 026 Batch 0040  Loss: 0.041051\n",
      "Epoch 026 Batch 0050  Loss: 0.009398\n",
      "Epoch 026 Batch 0060  Loss: 0.022649\n",
      "Epoch 026 Batch 0070  Loss: 0.051178\n",
      "Epoch 026 Batch 0080  Loss: 0.010859\n",
      "Epoch 026 Batch 0090  Loss: 0.028374\n",
      "Epoch 026 Batch 0100  Loss: 0.073159\n",
      "Epoch 026 Batch 0110  Loss: 0.061559\n",
      "Epoch 026 Batch 0120  Loss: 0.034821\n",
      "Epoch 026 Batch 0130  Loss: 0.023573\n",
      "Epoch 026 Batch 0140  Loss: 0.036784\n",
      "Epoch 026 Batch 0150  Loss: 0.032398\n",
      "Epoch 026 Batch 0160  Loss: 0.010077\n",
      "Epoch 026 Batch 0170  Loss: 0.024851\n",
      "Epoch 026 Batch 0180  Loss: 0.020867\n",
      "Epoch 026 Batch 0190  Loss: 0.051747\n",
      "Saved sample grid to ./samples/sample_epoch_026.png\n",
      "Epoch 027 Batch 0000  Loss: 0.021104\n",
      "Epoch 027 Batch 0010  Loss: 0.046626\n",
      "Epoch 027 Batch 0020  Loss: 0.049485\n",
      "Epoch 027 Batch 0030  Loss: 0.026262\n",
      "Epoch 027 Batch 0040  Loss: 0.041754\n",
      "Epoch 027 Batch 0050  Loss: 0.039510\n",
      "Epoch 027 Batch 0060  Loss: 0.038917\n",
      "Epoch 027 Batch 0070  Loss: 0.047991\n",
      "Epoch 027 Batch 0080  Loss: 0.039356\n",
      "Epoch 027 Batch 0090  Loss: 0.032032\n",
      "Epoch 027 Batch 0100  Loss: 0.021080\n",
      "Epoch 027 Batch 0110  Loss: 0.023152\n",
      "Epoch 027 Batch 0120  Loss: 0.033632\n",
      "Epoch 027 Batch 0130  Loss: 0.035604\n",
      "Epoch 027 Batch 0140  Loss: 0.019908\n",
      "Epoch 027 Batch 0150  Loss: 0.016843\n",
      "Epoch 027 Batch 0160  Loss: 0.041879\n",
      "Epoch 027 Batch 0170  Loss: 0.031563\n",
      "Epoch 027 Batch 0180  Loss: 0.016781\n",
      "Epoch 027 Batch 0190  Loss: 0.034074\n",
      "Saved sample grid to ./samples/sample_epoch_027.png\n",
      "Epoch 028 Batch 0000  Loss: 0.018363\n",
      "Epoch 028 Batch 0010  Loss: 0.036674\n",
      "Epoch 028 Batch 0020  Loss: 0.038125\n",
      "Epoch 028 Batch 0030  Loss: 0.035783\n",
      "Epoch 028 Batch 0040  Loss: 0.020756\n",
      "Epoch 028 Batch 0050  Loss: 0.012434\n",
      "Epoch 028 Batch 0060  Loss: 0.045175\n",
      "Epoch 028 Batch 0070  Loss: 0.030523\n",
      "Epoch 028 Batch 0080  Loss: 0.040586\n",
      "Epoch 028 Batch 0090  Loss: 0.039826\n",
      "Epoch 028 Batch 0100  Loss: 0.009650\n",
      "Epoch 028 Batch 0110  Loss: 0.047016\n",
      "Epoch 028 Batch 0120  Loss: 0.028404\n",
      "Epoch 028 Batch 0130  Loss: 0.024960\n",
      "Epoch 028 Batch 0140  Loss: 0.024991\n",
      "Epoch 028 Batch 0150  Loss: 0.042630\n",
      "Epoch 028 Batch 0160  Loss: 0.020813\n",
      "Epoch 028 Batch 0170  Loss: 0.018954\n",
      "Epoch 028 Batch 0180  Loss: 0.057184\n",
      "Epoch 028 Batch 0190  Loss: 0.017584\n",
      "Saved sample grid to ./samples/sample_epoch_028.png\n",
      "Epoch 029 Batch 0000  Loss: 0.029064\n",
      "Epoch 029 Batch 0010  Loss: 0.059303\n",
      "Epoch 029 Batch 0020  Loss: 0.049261\n",
      "Epoch 029 Batch 0030  Loss: 0.010149\n",
      "Epoch 029 Batch 0040  Loss: 0.024000\n",
      "Epoch 029 Batch 0050  Loss: 0.053218\n",
      "Epoch 029 Batch 0060  Loss: 0.025883\n",
      "Epoch 029 Batch 0070  Loss: 0.024192\n",
      "Epoch 029 Batch 0080  Loss: 0.035399\n",
      "Epoch 029 Batch 0090  Loss: 0.041959\n",
      "Epoch 029 Batch 0100  Loss: 0.010076\n",
      "Epoch 029 Batch 0110  Loss: 0.040290\n",
      "Epoch 029 Batch 0120  Loss: 0.037586\n",
      "Epoch 029 Batch 0130  Loss: 0.073370\n",
      "Epoch 029 Batch 0140  Loss: 0.025104\n",
      "Epoch 029 Batch 0150  Loss: 0.022456\n",
      "Epoch 029 Batch 0160  Loss: 0.040762\n",
      "Epoch 029 Batch 0170  Loss: 0.042636\n",
      "Epoch 029 Batch 0180  Loss: 0.052605\n",
      "Epoch 029 Batch 0190  Loss: 0.034954\n",
      "Saved sample grid to ./samples/sample_epoch_029.png\n",
      "Epoch 030 Batch 0000  Loss: 0.027200\n",
      "Epoch 030 Batch 0010  Loss: 0.051477\n",
      "Epoch 030 Batch 0020  Loss: 0.031965\n",
      "Epoch 030 Batch 0030  Loss: 0.045206\n",
      "Epoch 030 Batch 0040  Loss: 0.033810\n",
      "Epoch 030 Batch 0050  Loss: 0.021290\n",
      "Epoch 030 Batch 0060  Loss: 0.026950\n",
      "Epoch 030 Batch 0070  Loss: 0.044756\n",
      "Epoch 030 Batch 0080  Loss: 0.032414\n",
      "Epoch 030 Batch 0090  Loss: 0.024684\n",
      "Epoch 030 Batch 0100  Loss: 0.016851\n",
      "Epoch 030 Batch 0110  Loss: 0.009853\n",
      "Epoch 030 Batch 0120  Loss: 0.016028\n",
      "Epoch 030 Batch 0130  Loss: 0.016068\n",
      "Epoch 030 Batch 0140  Loss: 0.034819\n",
      "Epoch 030 Batch 0150  Loss: 0.013549\n",
      "Epoch 030 Batch 0160  Loss: 0.034642\n",
      "Epoch 030 Batch 0170  Loss: 0.033086\n",
      "Epoch 030 Batch 0180  Loss: 0.036341\n",
      "Epoch 030 Batch 0190  Loss: 0.044289\n",
      "Saved sample grid to ./samples/sample_epoch_030.png\n",
      "Epoch 031 Batch 0000  Loss: 0.027872\n",
      "Epoch 031 Batch 0010  Loss: 0.061561\n",
      "Epoch 031 Batch 0020  Loss: 0.022973\n",
      "Epoch 031 Batch 0030  Loss: 0.029893\n",
      "Epoch 031 Batch 0040  Loss: 0.039879\n",
      "Epoch 031 Batch 0050  Loss: 0.062649\n",
      "Epoch 031 Batch 0060  Loss: 0.020155\n",
      "Epoch 031 Batch 0070  Loss: 0.033795\n",
      "Epoch 031 Batch 0080  Loss: 0.030776\n",
      "Epoch 031 Batch 0090  Loss: 0.013550\n",
      "Epoch 031 Batch 0100  Loss: 0.014944\n",
      "Epoch 031 Batch 0110  Loss: 0.025880\n",
      "Epoch 031 Batch 0120  Loss: 0.033801\n",
      "Epoch 031 Batch 0130  Loss: 0.027751\n",
      "Epoch 031 Batch 0140  Loss: 0.015516\n",
      "Epoch 031 Batch 0150  Loss: 0.046962\n",
      "Epoch 031 Batch 0160  Loss: 0.070490\n",
      "Epoch 031 Batch 0170  Loss: 0.046105\n",
      "Epoch 031 Batch 0180  Loss: 0.029614\n",
      "Epoch 031 Batch 0190  Loss: 0.030501\n",
      "Saved sample grid to ./samples/sample_epoch_031.png\n",
      "Epoch 032 Batch 0000  Loss: 0.026187\n",
      "Epoch 032 Batch 0010  Loss: 0.059497\n",
      "Epoch 032 Batch 0020  Loss: 0.011643\n",
      "Epoch 032 Batch 0030  Loss: 0.026446\n",
      "Epoch 032 Batch 0040  Loss: 0.041853\n",
      "Epoch 032 Batch 0050  Loss: 0.020391\n",
      "Epoch 032 Batch 0060  Loss: 0.044414\n",
      "Epoch 032 Batch 0070  Loss: 0.025263\n",
      "Epoch 032 Batch 0080  Loss: 0.051294\n",
      "Epoch 032 Batch 0090  Loss: 0.013116\n",
      "Epoch 032 Batch 0100  Loss: 0.018201\n",
      "Epoch 032 Batch 0110  Loss: 0.019610\n",
      "Epoch 032 Batch 0120  Loss: 0.036698\n",
      "Epoch 032 Batch 0130  Loss: 0.015008\n",
      "Epoch 032 Batch 0140  Loss: 0.008147\n",
      "Epoch 032 Batch 0150  Loss: 0.019748\n",
      "Epoch 032 Batch 0160  Loss: 0.054678\n",
      "Epoch 032 Batch 0170  Loss: 0.022826\n",
      "Epoch 032 Batch 0180  Loss: 0.024819\n",
      "Epoch 032 Batch 0190  Loss: 0.015776\n",
      "Saved sample grid to ./samples/sample_epoch_032.png\n",
      "Epoch 033 Batch 0000  Loss: 0.036100\n",
      "Epoch 033 Batch 0010  Loss: 0.017095\n",
      "Epoch 033 Batch 0020  Loss: 0.030645\n",
      "Epoch 033 Batch 0030  Loss: 0.077526\n",
      "Epoch 033 Batch 0040  Loss: 0.021258\n",
      "Epoch 033 Batch 0050  Loss: 0.039864\n",
      "Epoch 033 Batch 0060  Loss: 0.033692\n",
      "Epoch 033 Batch 0070  Loss: 0.034681\n",
      "Epoch 033 Batch 0080  Loss: 0.017661\n",
      "Epoch 033 Batch 0090  Loss: 0.030263\n",
      "Epoch 033 Batch 0100  Loss: 0.030404\n",
      "Epoch 033 Batch 0110  Loss: 0.048131\n",
      "Epoch 033 Batch 0120  Loss: 0.026875\n",
      "Epoch 033 Batch 0130  Loss: 0.043086\n",
      "Epoch 033 Batch 0140  Loss: 0.019872\n",
      "Epoch 033 Batch 0150  Loss: 0.018405\n",
      "Epoch 033 Batch 0160  Loss: 0.085798\n",
      "Epoch 033 Batch 0170  Loss: 0.031253\n",
      "Epoch 033 Batch 0180  Loss: 0.042720\n",
      "Epoch 033 Batch 0190  Loss: 0.019795\n",
      "Saved sample grid to ./samples/sample_epoch_033.png\n",
      "Epoch 034 Batch 0000  Loss: 0.028207\n",
      "Epoch 034 Batch 0010  Loss: 0.019970\n",
      "Epoch 034 Batch 0020  Loss: 0.038528\n",
      "Epoch 034 Batch 0030  Loss: 0.048653\n",
      "Epoch 034 Batch 0040  Loss: 0.015539\n",
      "Epoch 034 Batch 0050  Loss: 0.008767\n",
      "Epoch 034 Batch 0060  Loss: 0.016638\n",
      "Epoch 034 Batch 0070  Loss: 0.017012\n",
      "Epoch 034 Batch 0080  Loss: 0.022668\n",
      "Epoch 034 Batch 0090  Loss: 0.030889\n",
      "Epoch 034 Batch 0100  Loss: 0.018417\n",
      "Epoch 034 Batch 0110  Loss: 0.027307\n",
      "Epoch 034 Batch 0120  Loss: 0.019202\n",
      "Epoch 034 Batch 0130  Loss: 0.025232\n",
      "Epoch 034 Batch 0140  Loss: 0.033790\n",
      "Epoch 034 Batch 0150  Loss: 0.027282\n",
      "Epoch 034 Batch 0160  Loss: 0.019949\n",
      "Epoch 034 Batch 0170  Loss: 0.020218\n",
      "Epoch 034 Batch 0180  Loss: 0.025654\n",
      "Epoch 034 Batch 0190  Loss: 0.054049\n",
      "Saved sample grid to ./samples/sample_epoch_034.png\n",
      "Epoch 035 Batch 0000  Loss: 0.036562\n",
      "Epoch 035 Batch 0010  Loss: 0.017248\n",
      "Epoch 035 Batch 0020  Loss: 0.011079\n",
      "Epoch 035 Batch 0030  Loss: 0.021075\n",
      "Epoch 035 Batch 0040  Loss: 0.023793\n",
      "Epoch 035 Batch 0050  Loss: 0.061502\n",
      "Epoch 035 Batch 0060  Loss: 0.031334\n",
      "Epoch 035 Batch 0070  Loss: 0.029697\n",
      "Epoch 035 Batch 0080  Loss: 0.032792\n",
      "Epoch 035 Batch 0090  Loss: 0.024790\n",
      "Epoch 035 Batch 0100  Loss: 0.010634\n",
      "Epoch 035 Batch 0110  Loss: 0.019291\n",
      "Epoch 035 Batch 0120  Loss: 0.020233\n",
      "Epoch 035 Batch 0130  Loss: 0.039775\n",
      "Epoch 035 Batch 0140  Loss: 0.024640\n",
      "Epoch 035 Batch 0150  Loss: 0.018451\n",
      "Epoch 035 Batch 0160  Loss: 0.020353\n",
      "Epoch 035 Batch 0170  Loss: 0.007817\n",
      "Epoch 035 Batch 0180  Loss: 0.048810\n",
      "Epoch 035 Batch 0190  Loss: 0.033998\n",
      "Saved sample grid to ./samples/sample_epoch_035.png\n",
      "Epoch 036 Batch 0000  Loss: 0.025229\n",
      "Epoch 036 Batch 0010  Loss: 0.035833\n",
      "Epoch 036 Batch 0020  Loss: 0.029261\n",
      "Epoch 036 Batch 0030  Loss: 0.025186\n",
      "Epoch 036 Batch 0040  Loss: 0.023542\n",
      "Epoch 036 Batch 0050  Loss: 0.033771\n",
      "Epoch 036 Batch 0060  Loss: 0.009914\n",
      "Epoch 036 Batch 0070  Loss: 0.029678\n",
      "Epoch 036 Batch 0080  Loss: 0.038724\n",
      "Epoch 036 Batch 0090  Loss: 0.015407\n",
      "Epoch 036 Batch 0100  Loss: 0.020161\n",
      "Epoch 036 Batch 0110  Loss: 0.025690\n",
      "Epoch 036 Batch 0120  Loss: 0.025594\n",
      "Epoch 036 Batch 0130  Loss: 0.026430\n",
      "Epoch 036 Batch 0140  Loss: 0.006210\n",
      "Epoch 036 Batch 0150  Loss: 0.036059\n",
      "Epoch 036 Batch 0160  Loss: 0.041434\n",
      "Epoch 036 Batch 0170  Loss: 0.030357\n",
      "Epoch 036 Batch 0180  Loss: 0.056595\n",
      "Epoch 036 Batch 0190  Loss: 0.025784\n",
      "Saved sample grid to ./samples/sample_epoch_036.png\n",
      "Epoch 037 Batch 0000  Loss: 0.015121\n",
      "Epoch 037 Batch 0010  Loss: 0.046368\n",
      "Epoch 037 Batch 0020  Loss: 0.013100\n",
      "Epoch 037 Batch 0030  Loss: 0.064474\n",
      "Epoch 037 Batch 0040  Loss: 0.026569\n",
      "Epoch 037 Batch 0050  Loss: 0.022164\n",
      "Epoch 037 Batch 0060  Loss: 0.035130\n",
      "Epoch 037 Batch 0070  Loss: 0.026899\n",
      "Epoch 037 Batch 0080  Loss: 0.033335\n",
      "Epoch 037 Batch 0090  Loss: 0.052942\n",
      "Epoch 037 Batch 0100  Loss: 0.027538\n",
      "Epoch 037 Batch 0110  Loss: 0.048606\n",
      "Epoch 037 Batch 0120  Loss: 0.015874\n",
      "Epoch 037 Batch 0130  Loss: 0.056976\n",
      "Epoch 037 Batch 0140  Loss: 0.052724\n",
      "Epoch 037 Batch 0150  Loss: 0.066664\n",
      "Epoch 037 Batch 0160  Loss: 0.022624\n",
      "Epoch 037 Batch 0170  Loss: 0.029780\n",
      "Epoch 037 Batch 0180  Loss: 0.029631\n",
      "Epoch 037 Batch 0190  Loss: 0.014456\n",
      "Saved sample grid to ./samples/sample_epoch_037.png\n",
      "Epoch 038 Batch 0000  Loss: 0.009281\n",
      "Epoch 038 Batch 0010  Loss: 0.021935\n",
      "Epoch 038 Batch 0020  Loss: 0.048074\n",
      "Epoch 038 Batch 0030  Loss: 0.023197\n",
      "Epoch 038 Batch 0040  Loss: 0.028426\n",
      "Epoch 038 Batch 0050  Loss: 0.023377\n",
      "Epoch 038 Batch 0060  Loss: 0.029497\n",
      "Epoch 038 Batch 0070  Loss: 0.061470\n",
      "Epoch 038 Batch 0080  Loss: 0.015640\n",
      "Epoch 038 Batch 0090  Loss: 0.011059\n",
      "Epoch 038 Batch 0100  Loss: 0.007169\n",
      "Epoch 038 Batch 0110  Loss: 0.041428\n",
      "Epoch 038 Batch 0120  Loss: 0.021679\n",
      "Epoch 038 Batch 0130  Loss: 0.007721\n",
      "Epoch 038 Batch 0140  Loss: 0.037936\n",
      "Epoch 038 Batch 0150  Loss: 0.019888\n",
      "Epoch 038 Batch 0160  Loss: 0.005682\n",
      "Epoch 038 Batch 0170  Loss: 0.023547\n",
      "Epoch 038 Batch 0180  Loss: 0.025463\n",
      "Epoch 038 Batch 0190  Loss: 0.033656\n",
      "Saved sample grid to ./samples/sample_epoch_038.png\n",
      "Epoch 039 Batch 0000  Loss: 0.053773\n",
      "Epoch 039 Batch 0010  Loss: 0.021664\n",
      "Epoch 039 Batch 0020  Loss: 0.019909\n",
      "Epoch 039 Batch 0030  Loss: 0.032982\n",
      "Epoch 039 Batch 0040  Loss: 0.016053\n",
      "Epoch 039 Batch 0050  Loss: 0.028333\n",
      "Epoch 039 Batch 0060  Loss: 0.025067\n",
      "Epoch 039 Batch 0070  Loss: 0.076845\n",
      "Epoch 039 Batch 0080  Loss: 0.064519\n",
      "Epoch 039 Batch 0090  Loss: 0.030216\n",
      "Epoch 039 Batch 0100  Loss: 0.049110\n",
      "Epoch 039 Batch 0110  Loss: 0.038378\n",
      "Epoch 039 Batch 0120  Loss: 0.034373\n",
      "Epoch 039 Batch 0130  Loss: 0.032751\n",
      "Epoch 039 Batch 0140  Loss: 0.054229\n",
      "Epoch 039 Batch 0150  Loss: 0.041851\n",
      "Epoch 039 Batch 0160  Loss: 0.043772\n",
      "Epoch 039 Batch 0170  Loss: 0.027171\n",
      "Epoch 039 Batch 0180  Loss: 0.012423\n",
      "Epoch 039 Batch 0190  Loss: 0.008902\n",
      "Saved sample grid to ./samples/sample_epoch_039.png\n",
      "Epoch 040 Batch 0000  Loss: 0.031702\n",
      "Epoch 040 Batch 0010  Loss: 0.024639\n",
      "Epoch 040 Batch 0020  Loss: 0.026974\n",
      "Epoch 040 Batch 0030  Loss: 0.012805\n",
      "Epoch 040 Batch 0040  Loss: 0.036967\n",
      "Epoch 040 Batch 0050  Loss: 0.039938\n",
      "Epoch 040 Batch 0060  Loss: 0.047804\n",
      "Epoch 040 Batch 0070  Loss: 0.028910\n",
      "Epoch 040 Batch 0080  Loss: 0.018211\n",
      "Epoch 040 Batch 0090  Loss: 0.028255\n",
      "Epoch 040 Batch 0100  Loss: 0.013754\n",
      "Epoch 040 Batch 0110  Loss: 0.022606\n",
      "Epoch 040 Batch 0120  Loss: 0.011645\n",
      "Epoch 040 Batch 0130  Loss: 0.034871\n",
      "Epoch 040 Batch 0140  Loss: 0.022418\n",
      "Epoch 040 Batch 0150  Loss: 0.017197\n",
      "Epoch 040 Batch 0160  Loss: 0.013550\n",
      "Epoch 040 Batch 0170  Loss: 0.009920\n",
      "Epoch 040 Batch 0180  Loss: 0.015923\n",
      "Epoch 040 Batch 0190  Loss: 0.045488\n",
      "Saved sample grid to ./samples/sample_epoch_040.png\n",
      "Epoch 041 Batch 0000  Loss: 0.014211\n",
      "Epoch 041 Batch 0010  Loss: 0.020245\n",
      "Epoch 041 Batch 0020  Loss: 0.060209\n",
      "Epoch 041 Batch 0030  Loss: 0.034056\n",
      "Epoch 041 Batch 0040  Loss: 0.036282\n",
      "Epoch 041 Batch 0050  Loss: 0.064637\n",
      "Epoch 041 Batch 0060  Loss: 0.035152\n",
      "Epoch 041 Batch 0070  Loss: 0.009744\n",
      "Epoch 041 Batch 0080  Loss: 0.026619\n",
      "Epoch 041 Batch 0090  Loss: 0.027719\n",
      "Epoch 041 Batch 0100  Loss: 0.034210\n",
      "Epoch 041 Batch 0110  Loss: 0.027417\n",
      "Epoch 041 Batch 0120  Loss: 0.044996\n",
      "Epoch 041 Batch 0130  Loss: 0.017028\n",
      "Epoch 041 Batch 0140  Loss: 0.051157\n",
      "Epoch 041 Batch 0150  Loss: 0.037972\n",
      "Epoch 041 Batch 0160  Loss: 0.032429\n",
      "Epoch 041 Batch 0170  Loss: 0.056777\n",
      "Epoch 041 Batch 0180  Loss: 0.019516\n",
      "Epoch 041 Batch 0190  Loss: 0.037420\n",
      "Saved sample grid to ./samples/sample_epoch_041.png\n",
      "Epoch 042 Batch 0000  Loss: 0.023969\n",
      "Epoch 042 Batch 0010  Loss: 0.016403\n",
      "Epoch 042 Batch 0020  Loss: 0.045496\n",
      "Epoch 042 Batch 0030  Loss: 0.024043\n",
      "Epoch 042 Batch 0040  Loss: 0.015813\n",
      "Epoch 042 Batch 0050  Loss: 0.013617\n",
      "Epoch 042 Batch 0060  Loss: 0.005854\n",
      "Epoch 042 Batch 0070  Loss: 0.018609\n",
      "Epoch 042 Batch 0080  Loss: 0.018412\n",
      "Epoch 042 Batch 0090  Loss: 0.061848\n",
      "Epoch 042 Batch 0100  Loss: 0.016699\n",
      "Epoch 042 Batch 0110  Loss: 0.018804\n",
      "Epoch 042 Batch 0120  Loss: 0.027118\n",
      "Epoch 042 Batch 0130  Loss: 0.035417\n",
      "Epoch 042 Batch 0140  Loss: 0.024037\n",
      "Epoch 042 Batch 0150  Loss: 0.008528\n",
      "Epoch 042 Batch 0160  Loss: 0.029681\n",
      "Epoch 042 Batch 0170  Loss: 0.014819\n",
      "Epoch 042 Batch 0180  Loss: 0.008534\n",
      "Epoch 042 Batch 0190  Loss: 0.023784\n",
      "Saved sample grid to ./samples/sample_epoch_042.png\n",
      "Epoch 043 Batch 0000  Loss: 0.010657\n",
      "Epoch 043 Batch 0010  Loss: 0.014314\n",
      "Epoch 043 Batch 0020  Loss: 0.036820\n",
      "Epoch 043 Batch 0030  Loss: 0.028745\n",
      "Epoch 043 Batch 0040  Loss: 0.054734\n",
      "Epoch 043 Batch 0050  Loss: 0.033033\n",
      "Epoch 043 Batch 0060  Loss: 0.061839\n",
      "Epoch 043 Batch 0070  Loss: 0.011587\n",
      "Epoch 043 Batch 0080  Loss: 0.040049\n",
      "Epoch 043 Batch 0090  Loss: 0.035401\n",
      "Epoch 043 Batch 0100  Loss: 0.036736\n",
      "Epoch 043 Batch 0110  Loss: 0.020053\n",
      "Epoch 043 Batch 0120  Loss: 0.013133\n",
      "Epoch 043 Batch 0130  Loss: 0.036009\n",
      "Epoch 043 Batch 0140  Loss: 0.025750\n",
      "Epoch 043 Batch 0150  Loss: 0.050503\n",
      "Epoch 043 Batch 0160  Loss: 0.018663\n",
      "Epoch 043 Batch 0170  Loss: 0.040051\n",
      "Epoch 043 Batch 0180  Loss: 0.014504\n",
      "Epoch 043 Batch 0190  Loss: 0.030229\n",
      "Saved sample grid to ./samples/sample_epoch_043.png\n",
      "Epoch 044 Batch 0000  Loss: 0.018640\n",
      "Epoch 044 Batch 0010  Loss: 0.012504\n",
      "Epoch 044 Batch 0020  Loss: 0.074928\n",
      "Epoch 044 Batch 0030  Loss: 0.019958\n",
      "Epoch 044 Batch 0040  Loss: 0.075911\n",
      "Epoch 044 Batch 0050  Loss: 0.056128\n",
      "Epoch 044 Batch 0060  Loss: 0.033678\n",
      "Epoch 044 Batch 0070  Loss: 0.022572\n",
      "Epoch 044 Batch 0080  Loss: 0.028363\n",
      "Epoch 044 Batch 0090  Loss: 0.015272\n",
      "Epoch 044 Batch 0100  Loss: 0.031583\n",
      "Epoch 044 Batch 0110  Loss: 0.027269\n",
      "Epoch 044 Batch 0120  Loss: 0.029910\n",
      "Epoch 044 Batch 0130  Loss: 0.015282\n",
      "Epoch 044 Batch 0140  Loss: 0.050815\n",
      "Epoch 044 Batch 0150  Loss: 0.034085\n",
      "Epoch 044 Batch 0160  Loss: 0.026634\n",
      "Epoch 044 Batch 0170  Loss: 0.014451\n",
      "Epoch 044 Batch 0180  Loss: 0.019621\n",
      "Epoch 044 Batch 0190  Loss: 0.029068\n",
      "Saved sample grid to ./samples/sample_epoch_044.png\n",
      "Epoch 045 Batch 0000  Loss: 0.019242\n",
      "Epoch 045 Batch 0010  Loss: 0.025348\n",
      "Epoch 045 Batch 0020  Loss: 0.025798\n",
      "Epoch 045 Batch 0030  Loss: 0.032381\n",
      "Epoch 045 Batch 0040  Loss: 0.029285\n",
      "Epoch 045 Batch 0050  Loss: 0.028938\n",
      "Epoch 045 Batch 0060  Loss: 0.034988\n",
      "Epoch 045 Batch 0070  Loss: 0.019458\n",
      "Epoch 045 Batch 0080  Loss: 0.016142\n",
      "Epoch 045 Batch 0090  Loss: 0.015262\n",
      "Epoch 045 Batch 0100  Loss: 0.025166\n",
      "Epoch 045 Batch 0110  Loss: 0.034250\n",
      "Epoch 045 Batch 0120  Loss: 0.017688\n",
      "Epoch 045 Batch 0130  Loss: 0.021326\n",
      "Epoch 045 Batch 0140  Loss: 0.039133\n",
      "Epoch 045 Batch 0150  Loss: 0.051648\n",
      "Epoch 045 Batch 0160  Loss: 0.038185\n",
      "Epoch 045 Batch 0170  Loss: 0.012375\n",
      "Epoch 045 Batch 0180  Loss: 0.015319\n",
      "Epoch 045 Batch 0190  Loss: 0.025834\n",
      "Saved sample grid to ./samples/sample_epoch_045.png\n",
      "Epoch 046 Batch 0000  Loss: 0.010472\n",
      "Epoch 046 Batch 0010  Loss: 0.045352\n",
      "Epoch 046 Batch 0020  Loss: 0.015626\n",
      "Epoch 046 Batch 0030  Loss: 0.027100\n",
      "Epoch 046 Batch 0040  Loss: 0.011812\n",
      "Epoch 046 Batch 0050  Loss: 0.019709\n",
      "Epoch 046 Batch 0060  Loss: 0.033122\n",
      "Epoch 046 Batch 0070  Loss: 0.020729\n",
      "Epoch 046 Batch 0080  Loss: 0.020868\n",
      "Epoch 046 Batch 0090  Loss: 0.017768\n",
      "Epoch 046 Batch 0100  Loss: 0.032394\n",
      "Epoch 046 Batch 0110  Loss: 0.014664\n",
      "Epoch 046 Batch 0120  Loss: 0.017438\n",
      "Epoch 046 Batch 0130  Loss: 0.026990\n",
      "Epoch 046 Batch 0140  Loss: 0.022017\n",
      "Epoch 046 Batch 0150  Loss: 0.027955\n",
      "Epoch 046 Batch 0160  Loss: 0.030504\n",
      "Epoch 046 Batch 0170  Loss: 0.040412\n",
      "Epoch 046 Batch 0180  Loss: 0.037137\n",
      "Epoch 046 Batch 0190  Loss: 0.020875\n",
      "Saved sample grid to ./samples/sample_epoch_046.png\n",
      "Epoch 047 Batch 0000  Loss: 0.031358\n",
      "Epoch 047 Batch 0010  Loss: 0.036888\n",
      "Epoch 047 Batch 0020  Loss: 0.013634\n",
      "Epoch 047 Batch 0030  Loss: 0.010032\n",
      "Epoch 047 Batch 0040  Loss: 0.020569\n",
      "Epoch 047 Batch 0050  Loss: 0.020507\n",
      "Epoch 047 Batch 0060  Loss: 0.034538\n",
      "Epoch 047 Batch 0070  Loss: 0.066955\n",
      "Epoch 047 Batch 0080  Loss: 0.016046\n",
      "Epoch 047 Batch 0090  Loss: 0.045177\n",
      "Epoch 047 Batch 0100  Loss: 0.023594\n",
      "Epoch 047 Batch 0110  Loss: 0.015301\n",
      "Epoch 047 Batch 0120  Loss: 0.029708\n",
      "Epoch 047 Batch 0130  Loss: 0.021360\n",
      "Epoch 047 Batch 0140  Loss: 0.029372\n",
      "Epoch 047 Batch 0150  Loss: 0.035950\n",
      "Epoch 047 Batch 0160  Loss: 0.044753\n",
      "Epoch 047 Batch 0170  Loss: 0.057419\n",
      "Epoch 047 Batch 0180  Loss: 0.014924\n",
      "Epoch 047 Batch 0190  Loss: 0.016018\n",
      "Saved sample grid to ./samples/sample_epoch_047.png\n",
      "Epoch 048 Batch 0000  Loss: 0.008713\n",
      "Epoch 048 Batch 0010  Loss: 0.018454\n",
      "Epoch 048 Batch 0020  Loss: 0.016212\n",
      "Epoch 048 Batch 0030  Loss: 0.037249\n",
      "Epoch 048 Batch 0040  Loss: 0.019093\n",
      "Epoch 048 Batch 0050  Loss: 0.013463\n",
      "Epoch 048 Batch 0060  Loss: 0.014834\n",
      "Epoch 048 Batch 0070  Loss: 0.012612\n",
      "Epoch 048 Batch 0080  Loss: 0.016317\n",
      "Epoch 048 Batch 0090  Loss: 0.015699\n",
      "Epoch 048 Batch 0100  Loss: 0.047553\n",
      "Epoch 048 Batch 0110  Loss: 0.045454\n",
      "Epoch 048 Batch 0120  Loss: 0.042897\n",
      "Epoch 048 Batch 0130  Loss: 0.021918\n",
      "Epoch 048 Batch 0140  Loss: 0.016548\n",
      "Epoch 048 Batch 0150  Loss: 0.026665\n",
      "Epoch 048 Batch 0160  Loss: 0.016228\n",
      "Epoch 048 Batch 0170  Loss: 0.031708\n",
      "Epoch 048 Batch 0180  Loss: 0.060981\n",
      "Epoch 048 Batch 0190  Loss: 0.015922\n",
      "Saved sample grid to ./samples/sample_epoch_048.png\n",
      "Epoch 049 Batch 0000  Loss: 0.029087\n",
      "Epoch 049 Batch 0010  Loss: 0.021665\n",
      "Epoch 049 Batch 0020  Loss: 0.030870\n",
      "Epoch 049 Batch 0030  Loss: 0.045407\n",
      "Epoch 049 Batch 0040  Loss: 0.030166\n",
      "Epoch 049 Batch 0050  Loss: 0.024041\n",
      "Epoch 049 Batch 0060  Loss: 0.024691\n",
      "Epoch 049 Batch 0070  Loss: 0.025900\n",
      "Epoch 049 Batch 0080  Loss: 0.020594\n",
      "Epoch 049 Batch 0090  Loss: 0.020380\n",
      "Epoch 049 Batch 0100  Loss: 0.027267\n",
      "Epoch 049 Batch 0110  Loss: 0.026988\n",
      "Epoch 049 Batch 0120  Loss: 0.024791\n",
      "Epoch 049 Batch 0130  Loss: 0.015371\n",
      "Epoch 049 Batch 0140  Loss: 0.040643\n",
      "Epoch 049 Batch 0150  Loss: 0.051323\n",
      "Epoch 049 Batch 0160  Loss: 0.030949\n",
      "Epoch 049 Batch 0170  Loss: 0.031339\n",
      "Epoch 049 Batch 0180  Loss: 0.021523\n",
      "Epoch 049 Batch 0190  Loss: 0.022884\n",
      "Saved sample grid to ./samples/sample_epoch_049.png\n",
      "Epoch 050 Batch 0000  Loss: 0.011806\n",
      "Epoch 050 Batch 0010  Loss: 0.036881\n",
      "Epoch 050 Batch 0020  Loss: 0.048560\n",
      "Epoch 050 Batch 0030  Loss: 0.013757\n",
      "Epoch 050 Batch 0040  Loss: 0.030907\n",
      "Epoch 050 Batch 0050  Loss: 0.021702\n",
      "Epoch 050 Batch 0060  Loss: 0.011833\n",
      "Epoch 050 Batch 0070  Loss: 0.025152\n",
      "Epoch 050 Batch 0080  Loss: 0.026222\n",
      "Epoch 050 Batch 0090  Loss: 0.020750\n",
      "Epoch 050 Batch 0100  Loss: 0.057252\n",
      "Epoch 050 Batch 0110  Loss: 0.021485\n",
      "Epoch 050 Batch 0120  Loss: 0.047887\n",
      "Epoch 050 Batch 0130  Loss: 0.023521\n",
      "Epoch 050 Batch 0140  Loss: 0.074798\n",
      "Epoch 050 Batch 0150  Loss: 0.023710\n",
      "Epoch 050 Batch 0160  Loss: 0.034787\n",
      "Epoch 050 Batch 0170  Loss: 0.052354\n",
      "Epoch 050 Batch 0180  Loss: 0.019019\n",
      "Epoch 050 Batch 0190  Loss: 0.054405\n",
      "Saved sample grid to ./samples/sample_epoch_050.png\n",
      "Epoch 051 Batch 0000  Loss: 0.018455\n",
      "Epoch 051 Batch 0010  Loss: 0.054143\n",
      "Epoch 051 Batch 0020  Loss: 0.014892\n",
      "Epoch 051 Batch 0030  Loss: 0.021320\n",
      "Epoch 051 Batch 0040  Loss: 0.045573\n",
      "Epoch 051 Batch 0050  Loss: 0.020296\n",
      "Epoch 051 Batch 0060  Loss: 0.028509\n",
      "Epoch 051 Batch 0070  Loss: 0.021155\n",
      "Epoch 051 Batch 0080  Loss: 0.039848\n",
      "Epoch 051 Batch 0090  Loss: 0.021801\n",
      "Epoch 051 Batch 0100  Loss: 0.024465\n",
      "Epoch 051 Batch 0110  Loss: 0.040085\n",
      "Epoch 051 Batch 0120  Loss: 0.016732\n",
      "Epoch 051 Batch 0130  Loss: 0.021705\n",
      "Epoch 051 Batch 0140  Loss: 0.031769\n",
      "Epoch 051 Batch 0150  Loss: 0.017760\n",
      "Epoch 051 Batch 0160  Loss: 0.033574\n",
      "Epoch 051 Batch 0170  Loss: 0.017745\n",
      "Epoch 051 Batch 0180  Loss: 0.022664\n",
      "Epoch 051 Batch 0190  Loss: 0.017716\n",
      "Saved sample grid to ./samples/sample_epoch_051.png\n",
      "Epoch 052 Batch 0000  Loss: 0.020458\n",
      "Epoch 052 Batch 0010  Loss: 0.021418\n",
      "Epoch 052 Batch 0020  Loss: 0.020388\n",
      "Epoch 052 Batch 0030  Loss: 0.031387\n",
      "Epoch 052 Batch 0040  Loss: 0.024743\n",
      "Epoch 052 Batch 0050  Loss: 0.083636\n",
      "Epoch 052 Batch 0060  Loss: 0.029536\n",
      "Epoch 052 Batch 0070  Loss: 0.016929\n",
      "Epoch 052 Batch 0080  Loss: 0.043634\n",
      "Epoch 052 Batch 0090  Loss: 0.021286\n",
      "Epoch 052 Batch 0100  Loss: 0.014425\n",
      "Epoch 052 Batch 0110  Loss: 0.028520\n",
      "Epoch 052 Batch 0120  Loss: 0.044921\n",
      "Epoch 052 Batch 0130  Loss: 0.036200\n",
      "Epoch 052 Batch 0140  Loss: 0.039987\n",
      "Epoch 052 Batch 0150  Loss: 0.025298\n",
      "Epoch 052 Batch 0160  Loss: 0.029045\n",
      "Epoch 052 Batch 0170  Loss: 0.027792\n",
      "Epoch 052 Batch 0180  Loss: 0.028144\n",
      "Epoch 052 Batch 0190  Loss: 0.015294\n",
      "Saved sample grid to ./samples/sample_epoch_052.png\n",
      "Epoch 053 Batch 0000  Loss: 0.025142\n",
      "Epoch 053 Batch 0010  Loss: 0.016652\n",
      "Epoch 053 Batch 0020  Loss: 0.025935\n",
      "Epoch 053 Batch 0030  Loss: 0.041205\n",
      "Epoch 053 Batch 0040  Loss: 0.006019\n",
      "Epoch 053 Batch 0050  Loss: 0.013488\n",
      "Epoch 053 Batch 0060  Loss: 0.037925\n",
      "Epoch 053 Batch 0070  Loss: 0.015102\n",
      "Epoch 053 Batch 0080  Loss: 0.031338\n",
      "Epoch 053 Batch 0090  Loss: 0.024148\n",
      "Epoch 053 Batch 0100  Loss: 0.031969\n",
      "Epoch 053 Batch 0110  Loss: 0.032226\n",
      "Epoch 053 Batch 0120  Loss: 0.009448\n",
      "Epoch 053 Batch 0130  Loss: 0.024850\n",
      "Epoch 053 Batch 0140  Loss: 0.019842\n",
      "Epoch 053 Batch 0150  Loss: 0.019626\n",
      "Epoch 053 Batch 0160  Loss: 0.029907\n",
      "Epoch 053 Batch 0170  Loss: 0.019092\n",
      "Epoch 053 Batch 0180  Loss: 0.014700\n",
      "Epoch 053 Batch 0190  Loss: 0.019525\n",
      "Saved sample grid to ./samples/sample_epoch_053.png\n",
      "Epoch 054 Batch 0000  Loss: 0.018729\n",
      "Epoch 054 Batch 0010  Loss: 0.014773\n",
      "Epoch 054 Batch 0020  Loss: 0.020030\n",
      "Epoch 054 Batch 0030  Loss: 0.015295\n",
      "Epoch 054 Batch 0040  Loss: 0.028912\n",
      "Epoch 054 Batch 0050  Loss: 0.026672\n",
      "Epoch 054 Batch 0060  Loss: 0.019112\n",
      "Epoch 054 Batch 0070  Loss: 0.034399\n",
      "Epoch 054 Batch 0080  Loss: 0.025324\n",
      "Epoch 054 Batch 0090  Loss: 0.014315\n",
      "Epoch 054 Batch 0100  Loss: 0.006450\n",
      "Epoch 054 Batch 0110  Loss: 0.032423\n",
      "Epoch 054 Batch 0120  Loss: 0.007457\n",
      "Epoch 054 Batch 0130  Loss: 0.005677\n",
      "Epoch 054 Batch 0140  Loss: 0.013736\n",
      "Epoch 054 Batch 0150  Loss: 0.024888\n",
      "Epoch 054 Batch 0160  Loss: 0.036664\n",
      "Epoch 054 Batch 0170  Loss: 0.053445\n",
      "Epoch 054 Batch 0180  Loss: 0.020297\n",
      "Epoch 054 Batch 0190  Loss: 0.013545\n",
      "Saved sample grid to ./samples/sample_epoch_054.png\n",
      "Epoch 055 Batch 0000  Loss: 0.012524\n",
      "Epoch 055 Batch 0010  Loss: 0.042083\n",
      "Epoch 055 Batch 0020  Loss: 0.047313\n",
      "Epoch 055 Batch 0030  Loss: 0.050837\n",
      "Epoch 055 Batch 0040  Loss: 0.039899\n",
      "Epoch 055 Batch 0050  Loss: 0.022639\n",
      "Epoch 055 Batch 0060  Loss: 0.046625\n",
      "Epoch 055 Batch 0070  Loss: 0.011621\n",
      "Epoch 055 Batch 0080  Loss: 0.014996\n",
      "Epoch 055 Batch 0090  Loss: 0.039195\n",
      "Epoch 055 Batch 0100  Loss: 0.013628\n",
      "Epoch 055 Batch 0110  Loss: 0.033818\n",
      "Epoch 055 Batch 0120  Loss: 0.026687\n",
      "Epoch 055 Batch 0130  Loss: 0.038549\n",
      "Epoch 055 Batch 0140  Loss: 0.017844\n",
      "Epoch 055 Batch 0150  Loss: 0.012280\n",
      "Epoch 055 Batch 0160  Loss: 0.017755\n",
      "Epoch 055 Batch 0170  Loss: 0.017599\n",
      "Epoch 055 Batch 0180  Loss: 0.043458\n",
      "Epoch 055 Batch 0190  Loss: 0.025920\n",
      "Saved sample grid to ./samples/sample_epoch_055.png\n",
      "Epoch 056 Batch 0000  Loss: 0.024301\n",
      "Epoch 056 Batch 0010  Loss: 0.010709\n",
      "Epoch 056 Batch 0020  Loss: 0.015024\n",
      "Epoch 056 Batch 0030  Loss: 0.022435\n",
      "Epoch 056 Batch 0040  Loss: 0.018266\n",
      "Epoch 056 Batch 0050  Loss: 0.021009\n",
      "Epoch 056 Batch 0060  Loss: 0.020716\n",
      "Epoch 056 Batch 0070  Loss: 0.033536\n",
      "Epoch 056 Batch 0080  Loss: 0.050814\n",
      "Epoch 056 Batch 0090  Loss: 0.024803\n",
      "Epoch 056 Batch 0100  Loss: 0.013083\n",
      "Epoch 056 Batch 0110  Loss: 0.026854\n",
      "Epoch 056 Batch 0120  Loss: 0.017647\n",
      "Epoch 056 Batch 0130  Loss: 0.016792\n",
      "Epoch 056 Batch 0140  Loss: 0.028492\n",
      "Epoch 056 Batch 0150  Loss: 0.013153\n",
      "Epoch 056 Batch 0160  Loss: 0.025946\n",
      "Epoch 056 Batch 0170  Loss: 0.011348\n",
      "Epoch 056 Batch 0180  Loss: 0.023604\n",
      "Epoch 056 Batch 0190  Loss: 0.018099\n",
      "Saved sample grid to ./samples/sample_epoch_056.png\n",
      "Epoch 057 Batch 0000  Loss: 0.024779\n",
      "Epoch 057 Batch 0010  Loss: 0.028564\n",
      "Epoch 057 Batch 0020  Loss: 0.016328\n",
      "Epoch 057 Batch 0030  Loss: 0.041502\n",
      "Epoch 057 Batch 0040  Loss: 0.023294\n",
      "Epoch 057 Batch 0050  Loss: 0.042301\n",
      "Epoch 057 Batch 0060  Loss: 0.035800\n",
      "Epoch 057 Batch 0070  Loss: 0.031861\n",
      "Epoch 057 Batch 0080  Loss: 0.070625\n",
      "Epoch 057 Batch 0090  Loss: 0.028214\n",
      "Epoch 057 Batch 0100  Loss: 0.010950\n",
      "Epoch 057 Batch 0110  Loss: 0.065798\n",
      "Epoch 057 Batch 0120  Loss: 0.063544\n",
      "Epoch 057 Batch 0130  Loss: 0.028037\n",
      "Epoch 057 Batch 0140  Loss: 0.026946\n",
      "Epoch 057 Batch 0150  Loss: 0.015918\n",
      "Epoch 057 Batch 0160  Loss: 0.018719\n",
      "Epoch 057 Batch 0170  Loss: 0.018820\n",
      "Epoch 057 Batch 0180  Loss: 0.047823\n",
      "Epoch 057 Batch 0190  Loss: 0.012272\n",
      "Saved sample grid to ./samples/sample_epoch_057.png\n",
      "Epoch 058 Batch 0000  Loss: 0.026629\n",
      "Epoch 058 Batch 0010  Loss: 0.025706\n",
      "Epoch 058 Batch 0020  Loss: 0.011410\n",
      "Epoch 058 Batch 0030  Loss: 0.014934\n",
      "Epoch 058 Batch 0040  Loss: 0.051080\n",
      "Epoch 058 Batch 0050  Loss: 0.038688\n",
      "Epoch 058 Batch 0060  Loss: 0.046621\n",
      "Epoch 058 Batch 0070  Loss: 0.030425\n",
      "Epoch 058 Batch 0080  Loss: 0.038267\n",
      "Epoch 058 Batch 0090  Loss: 0.030692\n",
      "Epoch 058 Batch 0100  Loss: 0.047873\n",
      "Epoch 058 Batch 0110  Loss: 0.031266\n",
      "Epoch 058 Batch 0120  Loss: 0.035133\n",
      "Epoch 058 Batch 0130  Loss: 0.022243\n",
      "Epoch 058 Batch 0140  Loss: 0.036419\n",
      "Epoch 058 Batch 0150  Loss: 0.051243\n",
      "Epoch 058 Batch 0160  Loss: 0.026085\n",
      "Epoch 058 Batch 0170  Loss: 0.035662\n",
      "Epoch 058 Batch 0180  Loss: 0.025157\n",
      "Epoch 058 Batch 0190  Loss: 0.014893\n",
      "Saved sample grid to ./samples/sample_epoch_058.png\n",
      "Epoch 059 Batch 0000  Loss: 0.034608\n",
      "Epoch 059 Batch 0010  Loss: 0.014661\n",
      "Epoch 059 Batch 0020  Loss: 0.030565\n",
      "Epoch 059 Batch 0030  Loss: 0.020822\n",
      "Epoch 059 Batch 0040  Loss: 0.046632\n",
      "Epoch 059 Batch 0050  Loss: 0.020624\n",
      "Epoch 059 Batch 0060  Loss: 0.020653\n",
      "Epoch 059 Batch 0070  Loss: 0.036705\n",
      "Epoch 059 Batch 0080  Loss: 0.033371\n",
      "Epoch 059 Batch 0090  Loss: 0.038243\n",
      "Epoch 059 Batch 0100  Loss: 0.025899\n",
      "Epoch 059 Batch 0110  Loss: 0.024998\n",
      "Epoch 059 Batch 0120  Loss: 0.027899\n",
      "Epoch 059 Batch 0130  Loss: 0.023049\n",
      "Epoch 059 Batch 0140  Loss: 0.020820\n",
      "Epoch 059 Batch 0150  Loss: 0.007927\n",
      "Epoch 059 Batch 0160  Loss: 0.056915\n",
      "Epoch 059 Batch 0170  Loss: 0.027066\n",
      "Epoch 059 Batch 0180  Loss: 0.030907\n",
      "Epoch 059 Batch 0190  Loss: 0.025871\n",
      "Saved sample grid to ./samples/sample_epoch_059.png\n",
      "Epoch 060 Batch 0000  Loss: 0.023163\n",
      "Epoch 060 Batch 0010  Loss: 0.029361\n",
      "Epoch 060 Batch 0020  Loss: 0.011107\n",
      "Epoch 060 Batch 0030  Loss: 0.015912\n",
      "Epoch 060 Batch 0040  Loss: 0.011395\n",
      "Epoch 060 Batch 0050  Loss: 0.016650\n",
      "Epoch 060 Batch 0060  Loss: 0.008267\n",
      "Epoch 060 Batch 0070  Loss: 0.035131\n",
      "Epoch 060 Batch 0080  Loss: 0.025139\n",
      "Epoch 060 Batch 0090  Loss: 0.014540\n",
      "Epoch 060 Batch 0100  Loss: 0.025934\n",
      "Epoch 060 Batch 0110  Loss: 0.018613\n",
      "Epoch 060 Batch 0120  Loss: 0.012319\n",
      "Epoch 060 Batch 0130  Loss: 0.022587\n",
      "Epoch 060 Batch 0140  Loss: 0.039549\n",
      "Epoch 060 Batch 0150  Loss: 0.040126\n",
      "Epoch 060 Batch 0160  Loss: 0.011548\n",
      "Epoch 060 Batch 0170  Loss: 0.021409\n",
      "Epoch 060 Batch 0180  Loss: 0.044133\n",
      "Epoch 060 Batch 0190  Loss: 0.040643\n",
      "Saved sample grid to ./samples/sample_epoch_060.png\n",
      "Epoch 061 Batch 0000  Loss: 0.065142\n",
      "Epoch 061 Batch 0010  Loss: 0.035408\n",
      "Epoch 061 Batch 0020  Loss: 0.056728\n",
      "Epoch 061 Batch 0030  Loss: 0.017219\n",
      "Epoch 061 Batch 0040  Loss: 0.028764\n",
      "Epoch 061 Batch 0050  Loss: 0.007927\n",
      "Epoch 061 Batch 0060  Loss: 0.014805\n",
      "Epoch 061 Batch 0070  Loss: 0.032357\n",
      "Epoch 061 Batch 0080  Loss: 0.020832\n",
      "Epoch 061 Batch 0090  Loss: 0.032183\n",
      "Epoch 061 Batch 0100  Loss: 0.025685\n",
      "Epoch 061 Batch 0110  Loss: 0.015632\n",
      "Epoch 061 Batch 0120  Loss: 0.012632\n",
      "Epoch 061 Batch 0130  Loss: 0.018440\n",
      "Epoch 061 Batch 0140  Loss: 0.028426\n",
      "Epoch 061 Batch 0150  Loss: 0.025929\n",
      "Epoch 061 Batch 0160  Loss: 0.046505\n",
      "Epoch 061 Batch 0170  Loss: 0.023600\n",
      "Epoch 061 Batch 0180  Loss: 0.011698\n",
      "Epoch 061 Batch 0190  Loss: 0.013632\n",
      "Saved sample grid to ./samples/sample_epoch_061.png\n",
      "Epoch 062 Batch 0000  Loss: 0.018622\n",
      "Epoch 062 Batch 0010  Loss: 0.019489\n",
      "Epoch 062 Batch 0020  Loss: 0.016016\n",
      "Epoch 062 Batch 0030  Loss: 0.026613\n",
      "Epoch 062 Batch 0040  Loss: 0.052969\n",
      "Epoch 062 Batch 0050  Loss: 0.025719\n",
      "Epoch 062 Batch 0060  Loss: 0.024727\n",
      "Epoch 062 Batch 0070  Loss: 0.023554\n",
      "Epoch 062 Batch 0080  Loss: 0.012828\n",
      "Epoch 062 Batch 0090  Loss: 0.040371\n",
      "Epoch 062 Batch 0100  Loss: 0.045708\n",
      "Epoch 062 Batch 0110  Loss: 0.017349\n",
      "Epoch 062 Batch 0120  Loss: 0.017578\n",
      "Epoch 062 Batch 0130  Loss: 0.029453\n",
      "Epoch 062 Batch 0140  Loss: 0.026211\n",
      "Epoch 062 Batch 0150  Loss: 0.043043\n",
      "Epoch 062 Batch 0160  Loss: 0.031692\n",
      "Epoch 062 Batch 0170  Loss: 0.028878\n",
      "Epoch 062 Batch 0180  Loss: 0.036928\n",
      "Epoch 062 Batch 0190  Loss: 0.008551\n",
      "Saved sample grid to ./samples/sample_epoch_062.png\n",
      "Epoch 063 Batch 0000  Loss: 0.034994\n",
      "Epoch 063 Batch 0010  Loss: 0.028289\n",
      "Epoch 063 Batch 0020  Loss: 0.022812\n",
      "Epoch 063 Batch 0030  Loss: 0.021366\n",
      "Epoch 063 Batch 0040  Loss: 0.016140\n",
      "Epoch 063 Batch 0050  Loss: 0.009531\n",
      "Epoch 063 Batch 0060  Loss: 0.033638\n",
      "Epoch 063 Batch 0070  Loss: 0.021523\n",
      "Epoch 063 Batch 0080  Loss: 0.054899\n",
      "Epoch 063 Batch 0090  Loss: 0.030136\n",
      "Epoch 063 Batch 0100  Loss: 0.041505\n",
      "Epoch 063 Batch 0110  Loss: 0.021369\n",
      "Epoch 063 Batch 0120  Loss: 0.022696\n",
      "Epoch 063 Batch 0130  Loss: 0.032427\n",
      "Epoch 063 Batch 0140  Loss: 0.028117\n",
      "Epoch 063 Batch 0150  Loss: 0.007814\n",
      "Epoch 063 Batch 0160  Loss: 0.019050\n",
      "Epoch 063 Batch 0170  Loss: 0.026629\n",
      "Epoch 063 Batch 0180  Loss: 0.017704\n",
      "Epoch 063 Batch 0190  Loss: 0.037537\n",
      "Saved sample grid to ./samples/sample_epoch_063.png\n",
      "Epoch 064 Batch 0000  Loss: 0.025324\n",
      "Epoch 064 Batch 0010  Loss: 0.010887\n",
      "Epoch 064 Batch 0020  Loss: 0.039304\n",
      "Epoch 064 Batch 0030  Loss: 0.011531\n",
      "Epoch 064 Batch 0040  Loss: 0.021389\n",
      "Epoch 064 Batch 0050  Loss: 0.019348\n",
      "Epoch 064 Batch 0060  Loss: 0.004965\n",
      "Epoch 064 Batch 0070  Loss: 0.016192\n",
      "Epoch 064 Batch 0080  Loss: 0.013290\n",
      "Epoch 064 Batch 0090  Loss: 0.046143\n",
      "Epoch 064 Batch 0100  Loss: 0.018550\n",
      "Epoch 064 Batch 0110  Loss: 0.015812\n",
      "Epoch 064 Batch 0120  Loss: 0.031682\n",
      "Epoch 064 Batch 0130  Loss: 0.049164\n",
      "Epoch 064 Batch 0140  Loss: 0.023464\n",
      "Epoch 064 Batch 0150  Loss: 0.015981\n",
      "Epoch 064 Batch 0160  Loss: 0.035251\n",
      "Epoch 064 Batch 0170  Loss: 0.057412\n",
      "Epoch 064 Batch 0180  Loss: 0.032173\n",
      "Epoch 064 Batch 0190  Loss: 0.014979\n",
      "Saved sample grid to ./samples/sample_epoch_064.png\n",
      "Epoch 065 Batch 0000  Loss: 0.052215\n",
      "Epoch 065 Batch 0010  Loss: 0.012547\n",
      "Epoch 065 Batch 0020  Loss: 0.094036\n",
      "Epoch 065 Batch 0030  Loss: 0.024048\n",
      "Epoch 065 Batch 0040  Loss: 0.027315\n",
      "Epoch 065 Batch 0050  Loss: 0.044046\n",
      "Epoch 065 Batch 0060  Loss: 0.034791\n",
      "Epoch 065 Batch 0070  Loss: 0.055672\n",
      "Epoch 065 Batch 0080  Loss: 0.016473\n",
      "Epoch 065 Batch 0090  Loss: 0.024339\n",
      "Epoch 065 Batch 0100  Loss: 0.012434\n",
      "Epoch 065 Batch 0110  Loss: 0.015670\n",
      "Epoch 065 Batch 0120  Loss: 0.022090\n",
      "Epoch 065 Batch 0130  Loss: 0.015893\n",
      "Epoch 065 Batch 0140  Loss: 0.020771\n",
      "Epoch 065 Batch 0150  Loss: 0.008707\n",
      "Epoch 065 Batch 0160  Loss: 0.022760\n",
      "Epoch 065 Batch 0170  Loss: 0.038920\n",
      "Epoch 065 Batch 0180  Loss: 0.029950\n",
      "Epoch 065 Batch 0190  Loss: 0.038115\n",
      "Saved sample grid to ./samples/sample_epoch_065.png\n",
      "Epoch 066 Batch 0000  Loss: 0.018961\n",
      "Epoch 066 Batch 0010  Loss: 0.038254\n",
      "Epoch 066 Batch 0020  Loss: 0.032808\n",
      "Epoch 066 Batch 0030  Loss: 0.022211\n",
      "Epoch 066 Batch 0040  Loss: 0.028386\n",
      "Epoch 066 Batch 0050  Loss: 0.038366\n",
      "Epoch 066 Batch 0060  Loss: 0.025912\n",
      "Epoch 066 Batch 0070  Loss: 0.036899\n",
      "Epoch 066 Batch 0080  Loss: 0.014323\n",
      "Epoch 066 Batch 0090  Loss: 0.025436\n",
      "Epoch 066 Batch 0100  Loss: 0.048323\n",
      "Epoch 066 Batch 0110  Loss: 0.044954\n",
      "Epoch 066 Batch 0120  Loss: 0.030234\n",
      "Epoch 066 Batch 0130  Loss: 0.012702\n",
      "Epoch 066 Batch 0140  Loss: 0.013112\n",
      "Epoch 066 Batch 0150  Loss: 0.019206\n",
      "Epoch 066 Batch 0160  Loss: 0.029370\n",
      "Epoch 066 Batch 0170  Loss: 0.020187\n",
      "Epoch 066 Batch 0180  Loss: 0.037568\n",
      "Epoch 066 Batch 0190  Loss: 0.070925\n",
      "Saved sample grid to ./samples/sample_epoch_066.png\n",
      "Epoch 067 Batch 0000  Loss: 0.045723\n",
      "Epoch 067 Batch 0010  Loss: 0.083868\n",
      "Epoch 067 Batch 0020  Loss: 0.029994\n",
      "Epoch 067 Batch 0030  Loss: 0.054968\n",
      "Epoch 067 Batch 0040  Loss: 0.026924\n",
      "Epoch 067 Batch 0050  Loss: 0.005998\n",
      "Epoch 067 Batch 0060  Loss: 0.024922\n",
      "Epoch 067 Batch 0070  Loss: 0.038789\n",
      "Epoch 067 Batch 0080  Loss: 0.023825\n",
      "Epoch 067 Batch 0090  Loss: 0.026312\n",
      "Epoch 067 Batch 0100  Loss: 0.027707\n",
      "Epoch 067 Batch 0110  Loss: 0.031858\n",
      "Epoch 067 Batch 0120  Loss: 0.035879\n",
      "Epoch 067 Batch 0130  Loss: 0.014270\n",
      "Epoch 067 Batch 0140  Loss: 0.045355\n",
      "Epoch 067 Batch 0150  Loss: 0.023873\n",
      "Epoch 067 Batch 0160  Loss: 0.017175\n",
      "Epoch 067 Batch 0170  Loss: 0.044318\n",
      "Epoch 067 Batch 0180  Loss: 0.022519\n",
      "Epoch 067 Batch 0190  Loss: 0.022305\n",
      "Saved sample grid to ./samples/sample_epoch_067.png\n",
      "Epoch 068 Batch 0000  Loss: 0.040959\n",
      "Epoch 068 Batch 0010  Loss: 0.035966\n",
      "Epoch 068 Batch 0020  Loss: 0.013107\n",
      "Epoch 068 Batch 0030  Loss: 0.014144\n",
      "Epoch 068 Batch 0040  Loss: 0.014229\n",
      "Epoch 068 Batch 0050  Loss: 0.018238\n",
      "Epoch 068 Batch 0060  Loss: 0.026889\n",
      "Epoch 068 Batch 0070  Loss: 0.006121\n",
      "Epoch 068 Batch 0080  Loss: 0.017348\n",
      "Epoch 068 Batch 0090  Loss: 0.032544\n",
      "Epoch 068 Batch 0100  Loss: 0.014897\n",
      "Epoch 068 Batch 0110  Loss: 0.011195\n",
      "Epoch 068 Batch 0120  Loss: 0.015225\n",
      "Epoch 068 Batch 0130  Loss: 0.029197\n",
      "Epoch 068 Batch 0140  Loss: 0.010786\n",
      "Epoch 068 Batch 0150  Loss: 0.019147\n",
      "Epoch 068 Batch 0160  Loss: 0.028270\n",
      "Epoch 068 Batch 0170  Loss: 0.010157\n",
      "Epoch 068 Batch 0180  Loss: 0.015176\n",
      "Epoch 068 Batch 0190  Loss: 0.030380\n",
      "Saved sample grid to ./samples/sample_epoch_068.png\n",
      "Epoch 069 Batch 0000  Loss: 0.027900\n",
      "Epoch 069 Batch 0010  Loss: 0.028207\n",
      "Epoch 069 Batch 0020  Loss: 0.026197\n",
      "Epoch 069 Batch 0030  Loss: 0.039847\n",
      "Epoch 069 Batch 0040  Loss: 0.019079\n",
      "Epoch 069 Batch 0050  Loss: 0.041030\n",
      "Epoch 069 Batch 0060  Loss: 0.030327\n",
      "Epoch 069 Batch 0070  Loss: 0.014843\n",
      "Epoch 069 Batch 0080  Loss: 0.019420\n",
      "Epoch 069 Batch 0090  Loss: 0.012415\n",
      "Epoch 069 Batch 0100  Loss: 0.028952\n",
      "Epoch 069 Batch 0110  Loss: 0.034757\n",
      "Epoch 069 Batch 0120  Loss: 0.017583\n",
      "Epoch 069 Batch 0130  Loss: 0.008735\n",
      "Epoch 069 Batch 0140  Loss: 0.027046\n",
      "Epoch 069 Batch 0150  Loss: 0.018137\n",
      "Epoch 069 Batch 0160  Loss: 0.019659\n",
      "Epoch 069 Batch 0170  Loss: 0.021203\n",
      "Epoch 069 Batch 0180  Loss: 0.029869\n",
      "Epoch 069 Batch 0190  Loss: 0.010479\n",
      "Saved sample grid to ./samples/sample_epoch_069.png\n",
      "Epoch 070 Batch 0000  Loss: 0.044446\n",
      "Epoch 070 Batch 0010  Loss: 0.025717\n",
      "Epoch 070 Batch 0020  Loss: 0.014957\n",
      "Epoch 070 Batch 0030  Loss: 0.020055\n",
      "Epoch 070 Batch 0040  Loss: 0.009033\n",
      "Epoch 070 Batch 0050  Loss: 0.020089\n",
      "Epoch 070 Batch 0060  Loss: 0.014991\n",
      "Epoch 070 Batch 0070  Loss: 0.067652\n",
      "Epoch 070 Batch 0080  Loss: 0.040560\n",
      "Epoch 070 Batch 0090  Loss: 0.014160\n",
      "Epoch 070 Batch 0100  Loss: 0.017268\n",
      "Epoch 070 Batch 0110  Loss: 0.025718\n",
      "Epoch 070 Batch 0120  Loss: 0.046207\n",
      "Epoch 070 Batch 0130  Loss: 0.023540\n",
      "Epoch 070 Batch 0140  Loss: 0.029014\n",
      "Epoch 070 Batch 0150  Loss: 0.034844\n",
      "Epoch 070 Batch 0160  Loss: 0.034833\n",
      "Epoch 070 Batch 0170  Loss: 0.015832\n",
      "Epoch 070 Batch 0180  Loss: 0.028145\n",
      "Epoch 070 Batch 0190  Loss: 0.010039\n",
      "Saved sample grid to ./samples/sample_epoch_070.png\n",
      "Epoch 071 Batch 0000  Loss: 0.020688\n",
      "Epoch 071 Batch 0010  Loss: 0.027889\n",
      "Epoch 071 Batch 0020  Loss: 0.057411\n",
      "Epoch 071 Batch 0030  Loss: 0.040290\n",
      "Epoch 071 Batch 0040  Loss: 0.039575\n",
      "Epoch 071 Batch 0050  Loss: 0.020540\n",
      "Epoch 071 Batch 0060  Loss: 0.021475\n",
      "Epoch 071 Batch 0070  Loss: 0.020494\n",
      "Epoch 071 Batch 0080  Loss: 0.029751\n",
      "Epoch 071 Batch 0090  Loss: 0.021745\n",
      "Epoch 071 Batch 0100  Loss: 0.025280\n",
      "Epoch 071 Batch 0110  Loss: 0.023117\n",
      "Epoch 071 Batch 0120  Loss: 0.019204\n",
      "Epoch 071 Batch 0130  Loss: 0.021175\n",
      "Epoch 071 Batch 0140  Loss: 0.004973\n",
      "Epoch 071 Batch 0150  Loss: 0.014940\n",
      "Epoch 071 Batch 0160  Loss: 0.022928\n",
      "Epoch 071 Batch 0170  Loss: 0.024589\n",
      "Epoch 071 Batch 0180  Loss: 0.022479\n",
      "Epoch 071 Batch 0190  Loss: 0.025572\n",
      "Saved sample grid to ./samples/sample_epoch_071.png\n",
      "Epoch 072 Batch 0000  Loss: 0.081608\n",
      "Epoch 072 Batch 0010  Loss: 0.029019\n",
      "Epoch 072 Batch 0020  Loss: 0.019334\n",
      "Epoch 072 Batch 0030  Loss: 0.036233\n",
      "Epoch 072 Batch 0040  Loss: 0.020428\n",
      "Epoch 072 Batch 0050  Loss: 0.012328\n",
      "Epoch 072 Batch 0060  Loss: 0.052171\n",
      "Epoch 072 Batch 0070  Loss: 0.025161\n",
      "Epoch 072 Batch 0080  Loss: 0.019509\n",
      "Epoch 072 Batch 0090  Loss: 0.059954\n",
      "Epoch 072 Batch 0100  Loss: 0.024696\n",
      "Epoch 072 Batch 0110  Loss: 0.013776\n",
      "Epoch 072 Batch 0120  Loss: 0.055217\n",
      "Epoch 072 Batch 0130  Loss: 0.020896\n",
      "Epoch 072 Batch 0140  Loss: 0.005675\n",
      "Epoch 072 Batch 0150  Loss: 0.034940\n",
      "Epoch 072 Batch 0160  Loss: 0.046708\n",
      "Epoch 072 Batch 0170  Loss: 0.034942\n",
      "Epoch 072 Batch 0180  Loss: 0.023254\n",
      "Epoch 072 Batch 0190  Loss: 0.028001\n",
      "Saved sample grid to ./samples/sample_epoch_072.png\n",
      "Epoch 073 Batch 0000  Loss: 0.037420\n",
      "Epoch 073 Batch 0010  Loss: 0.010378\n",
      "Epoch 073 Batch 0020  Loss: 0.013794\n",
      "Epoch 073 Batch 0030  Loss: 0.020631\n",
      "Epoch 073 Batch 0040  Loss: 0.015644\n",
      "Epoch 073 Batch 0050  Loss: 0.037785\n",
      "Epoch 073 Batch 0060  Loss: 0.067214\n",
      "Epoch 073 Batch 0070  Loss: 0.026804\n",
      "Epoch 073 Batch 0080  Loss: 0.041181\n",
      "Epoch 073 Batch 0090  Loss: 0.014243\n",
      "Epoch 073 Batch 0100  Loss: 0.009763\n",
      "Epoch 073 Batch 0110  Loss: 0.025635\n",
      "Epoch 073 Batch 0120  Loss: 0.022750\n",
      "Epoch 073 Batch 0130  Loss: 0.029394\n",
      "Epoch 073 Batch 0140  Loss: 0.052930\n",
      "Epoch 073 Batch 0150  Loss: 0.036464\n",
      "Epoch 073 Batch 0160  Loss: 0.023363\n",
      "Epoch 073 Batch 0170  Loss: 0.008197\n",
      "Epoch 073 Batch 0180  Loss: 0.018971\n",
      "Epoch 073 Batch 0190  Loss: 0.015747\n",
      "Saved sample grid to ./samples/sample_epoch_073.png\n",
      "Epoch 074 Batch 0000  Loss: 0.027637\n",
      "Epoch 074 Batch 0010  Loss: 0.041092\n",
      "Epoch 074 Batch 0020  Loss: 0.031680\n",
      "Epoch 074 Batch 0030  Loss: 0.042769\n",
      "Epoch 074 Batch 0040  Loss: 0.034917\n",
      "Epoch 074 Batch 0050  Loss: 0.011634\n",
      "Epoch 074 Batch 0060  Loss: 0.022930\n",
      "Epoch 074 Batch 0070  Loss: 0.021178\n",
      "Epoch 074 Batch 0080  Loss: 0.030352\n",
      "Epoch 074 Batch 0090  Loss: 0.028816\n",
      "Epoch 074 Batch 0100  Loss: 0.019109\n",
      "Epoch 074 Batch 0110  Loss: 0.034283\n",
      "Epoch 074 Batch 0120  Loss: 0.030743\n",
      "Epoch 074 Batch 0130  Loss: 0.018976\n",
      "Epoch 074 Batch 0140  Loss: 0.065487\n",
      "Epoch 074 Batch 0150  Loss: 0.013153\n",
      "Epoch 074 Batch 0160  Loss: 0.024642\n",
      "Epoch 074 Batch 0170  Loss: 0.068256\n",
      "Epoch 074 Batch 0180  Loss: 0.015305\n",
      "Epoch 074 Batch 0190  Loss: 0.015639\n",
      "Saved sample grid to ./samples/sample_epoch_074.png\n",
      "Epoch 075 Batch 0000  Loss: 0.011484\n",
      "Epoch 075 Batch 0010  Loss: 0.005674\n",
      "Epoch 075 Batch 0020  Loss: 0.015748\n",
      "Epoch 075 Batch 0030  Loss: 0.017334\n",
      "Epoch 075 Batch 0040  Loss: 0.025993\n",
      "Epoch 075 Batch 0050  Loss: 0.019387\n",
      "Epoch 075 Batch 0060  Loss: 0.038962\n",
      "Epoch 075 Batch 0070  Loss: 0.020732\n",
      "Epoch 075 Batch 0080  Loss: 0.022361\n",
      "Epoch 075 Batch 0090  Loss: 0.022117\n",
      "Epoch 075 Batch 0100  Loss: 0.049484\n",
      "Epoch 075 Batch 0110  Loss: 0.041852\n",
      "Epoch 075 Batch 0120  Loss: 0.027441\n",
      "Epoch 075 Batch 0130  Loss: 0.014732\n",
      "Epoch 075 Batch 0140  Loss: 0.032931\n",
      "Epoch 075 Batch 0150  Loss: 0.040784\n",
      "Epoch 075 Batch 0160  Loss: 0.010862\n",
      "Epoch 075 Batch 0170  Loss: 0.031154\n",
      "Epoch 075 Batch 0180  Loss: 0.015483\n",
      "Epoch 075 Batch 0190  Loss: 0.020995\n",
      "Saved sample grid to ./samples/sample_epoch_075.png\n",
      "Epoch 076 Batch 0000  Loss: 0.023446\n",
      "Epoch 076 Batch 0010  Loss: 0.032561\n",
      "Epoch 076 Batch 0020  Loss: 0.025289\n",
      "Epoch 076 Batch 0030  Loss: 0.026613\n",
      "Epoch 076 Batch 0040  Loss: 0.023324\n",
      "Epoch 076 Batch 0050  Loss: 0.021887\n",
      "Epoch 076 Batch 0060  Loss: 0.034372\n",
      "Epoch 076 Batch 0070  Loss: 0.040145\n",
      "Epoch 076 Batch 0080  Loss: 0.019469\n",
      "Epoch 076 Batch 0090  Loss: 0.017306\n",
      "Epoch 076 Batch 0100  Loss: 0.036483\n",
      "Epoch 076 Batch 0110  Loss: 0.007887\n",
      "Epoch 076 Batch 0120  Loss: 0.018879\n",
      "Epoch 076 Batch 0130  Loss: 0.029631\n",
      "Epoch 076 Batch 0140  Loss: 0.019808\n",
      "Epoch 076 Batch 0150  Loss: 0.024393\n",
      "Epoch 076 Batch 0160  Loss: 0.031451\n",
      "Epoch 076 Batch 0170  Loss: 0.013331\n",
      "Epoch 076 Batch 0180  Loss: 0.023863\n",
      "Epoch 076 Batch 0190  Loss: 0.041326\n",
      "Saved sample grid to ./samples/sample_epoch_076.png\n",
      "Epoch 077 Batch 0000  Loss: 0.024269\n",
      "Epoch 077 Batch 0010  Loss: 0.021737\n",
      "Epoch 077 Batch 0020  Loss: 0.013683\n",
      "Epoch 077 Batch 0030  Loss: 0.018677\n",
      "Epoch 077 Batch 0040  Loss: 0.024467\n",
      "Epoch 077 Batch 0050  Loss: 0.031910\n",
      "Epoch 077 Batch 0060  Loss: 0.021754\n",
      "Epoch 077 Batch 0070  Loss: 0.027910\n",
      "Epoch 077 Batch 0080  Loss: 0.041139\n",
      "Epoch 077 Batch 0090  Loss: 0.031074\n",
      "Epoch 077 Batch 0100  Loss: 0.015991\n",
      "Epoch 077 Batch 0110  Loss: 0.024674\n",
      "Epoch 077 Batch 0120  Loss: 0.018701\n",
      "Epoch 077 Batch 0130  Loss: 0.017446\n",
      "Epoch 077 Batch 0140  Loss: 0.052449\n",
      "Epoch 077 Batch 0150  Loss: 0.016220\n",
      "Epoch 077 Batch 0160  Loss: 0.026517\n",
      "Epoch 077 Batch 0170  Loss: 0.011476\n",
      "Epoch 077 Batch 0180  Loss: 0.030927\n",
      "Epoch 077 Batch 0190  Loss: 0.032331\n",
      "Saved sample grid to ./samples/sample_epoch_077.png\n",
      "Epoch 078 Batch 0000  Loss: 0.028884\n",
      "Epoch 078 Batch 0010  Loss: 0.028316\n",
      "Epoch 078 Batch 0020  Loss: 0.028115\n",
      "Epoch 078 Batch 0030  Loss: 0.031015\n",
      "Epoch 078 Batch 0040  Loss: 0.015194\n",
      "Epoch 078 Batch 0050  Loss: 0.009876\n",
      "Epoch 078 Batch 0060  Loss: 0.052151\n",
      "Epoch 078 Batch 0070  Loss: 0.021748\n",
      "Epoch 078 Batch 0080  Loss: 0.027986\n",
      "Epoch 078 Batch 0090  Loss: 0.026439\n",
      "Epoch 078 Batch 0100  Loss: 0.018364\n",
      "Epoch 078 Batch 0110  Loss: 0.034271\n",
      "Epoch 078 Batch 0120  Loss: 0.043043\n",
      "Epoch 078 Batch 0130  Loss: 0.019045\n",
      "Epoch 078 Batch 0140  Loss: 0.018063\n",
      "Epoch 078 Batch 0150  Loss: 0.012856\n",
      "Epoch 078 Batch 0160  Loss: 0.036294\n",
      "Epoch 078 Batch 0170  Loss: 0.034255\n",
      "Epoch 078 Batch 0180  Loss: 0.013774\n",
      "Epoch 078 Batch 0190  Loss: 0.028856\n",
      "Saved sample grid to ./samples/sample_epoch_078.png\n",
      "Epoch 079 Batch 0000  Loss: 0.031196\n",
      "Epoch 079 Batch 0010  Loss: 0.024350\n",
      "Epoch 079 Batch 0020  Loss: 0.022304\n",
      "Epoch 079 Batch 0030  Loss: 0.015890\n",
      "Epoch 079 Batch 0040  Loss: 0.055022\n",
      "Epoch 079 Batch 0050  Loss: 0.013921\n",
      "Epoch 079 Batch 0060  Loss: 0.031817\n",
      "Epoch 079 Batch 0070  Loss: 0.032150\n",
      "Epoch 079 Batch 0080  Loss: 0.033137\n",
      "Epoch 079 Batch 0090  Loss: 0.029776\n",
      "Epoch 079 Batch 0100  Loss: 0.031362\n",
      "Epoch 079 Batch 0110  Loss: 0.039535\n",
      "Epoch 079 Batch 0120  Loss: 0.029157\n",
      "Epoch 079 Batch 0130  Loss: 0.015097\n",
      "Epoch 079 Batch 0140  Loss: 0.033017\n",
      "Epoch 079 Batch 0150  Loss: 0.014164\n",
      "Epoch 079 Batch 0160  Loss: 0.022431\n",
      "Epoch 079 Batch 0170  Loss: 0.019781\n",
      "Epoch 079 Batch 0180  Loss: 0.015334\n",
      "Epoch 079 Batch 0190  Loss: 0.019788\n",
      "Saved sample grid to ./samples/sample_epoch_079.png\n",
      "Epoch 080 Batch 0000  Loss: 0.049663\n",
      "Epoch 080 Batch 0010  Loss: 0.013674\n",
      "Epoch 080 Batch 0020  Loss: 0.034379\n",
      "Epoch 080 Batch 0030  Loss: 0.029648\n",
      "Epoch 080 Batch 0040  Loss: 0.042992\n",
      "Epoch 080 Batch 0050  Loss: 0.026257\n",
      "Epoch 080 Batch 0060  Loss: 0.019143\n",
      "Epoch 080 Batch 0070  Loss: 0.039082\n",
      "Epoch 080 Batch 0080  Loss: 0.023893\n",
      "Epoch 080 Batch 0090  Loss: 0.030261\n",
      "Epoch 080 Batch 0100  Loss: 0.036715\n",
      "Epoch 080 Batch 0110  Loss: 0.015646\n",
      "Epoch 080 Batch 0120  Loss: 0.028624\n",
      "Epoch 080 Batch 0130  Loss: 0.015785\n",
      "Epoch 080 Batch 0140  Loss: 0.020808\n",
      "Epoch 080 Batch 0150  Loss: 0.014220\n",
      "Epoch 080 Batch 0160  Loss: 0.015965\n",
      "Epoch 080 Batch 0170  Loss: 0.011218\n",
      "Epoch 080 Batch 0180  Loss: 0.062827\n",
      "Epoch 080 Batch 0190  Loss: 0.024928\n",
      "Saved sample grid to ./samples/sample_epoch_080.png\n",
      "Epoch 081 Batch 0000  Loss: 0.042670\n",
      "Epoch 081 Batch 0010  Loss: 0.024659\n",
      "Epoch 081 Batch 0020  Loss: 0.019487\n",
      "Epoch 081 Batch 0030  Loss: 0.062999\n",
      "Epoch 081 Batch 0040  Loss: 0.038471\n",
      "Epoch 081 Batch 0050  Loss: 0.047937\n",
      "Epoch 081 Batch 0060  Loss: 0.026862\n",
      "Epoch 081 Batch 0070  Loss: 0.022331\n",
      "Epoch 081 Batch 0080  Loss: 0.020873\n",
      "Epoch 081 Batch 0090  Loss: 0.021427\n",
      "Epoch 081 Batch 0100  Loss: 0.026558\n",
      "Epoch 081 Batch 0110  Loss: 0.007546\n",
      "Epoch 081 Batch 0120  Loss: 0.026114\n",
      "Epoch 081 Batch 0130  Loss: 0.020252\n",
      "Epoch 081 Batch 0140  Loss: 0.022757\n",
      "Epoch 081 Batch 0150  Loss: 0.011604\n",
      "Epoch 081 Batch 0160  Loss: 0.019534\n",
      "Epoch 081 Batch 0170  Loss: 0.022401\n",
      "Epoch 081 Batch 0180  Loss: 0.031580\n",
      "Epoch 081 Batch 0190  Loss: 0.017515\n",
      "Saved sample grid to ./samples/sample_epoch_081.png\n",
      "Epoch 082 Batch 0000  Loss: 0.016320\n",
      "Epoch 082 Batch 0010  Loss: 0.013643\n",
      "Epoch 082 Batch 0020  Loss: 0.011661\n",
      "Epoch 082 Batch 0030  Loss: 0.053363\n",
      "Epoch 082 Batch 0040  Loss: 0.009594\n",
      "Epoch 082 Batch 0050  Loss: 0.024314\n",
      "Epoch 082 Batch 0060  Loss: 0.038508\n",
      "Epoch 082 Batch 0070  Loss: 0.029841\n",
      "Epoch 082 Batch 0080  Loss: 0.025497\n",
      "Epoch 082 Batch 0090  Loss: 0.035776\n",
      "Epoch 082 Batch 0100  Loss: 0.012190\n",
      "Epoch 082 Batch 0110  Loss: 0.030321\n",
      "Epoch 082 Batch 0120  Loss: 0.025691\n",
      "Epoch 082 Batch 0130  Loss: 0.020693\n",
      "Epoch 082 Batch 0140  Loss: 0.011844\n",
      "Epoch 082 Batch 0150  Loss: 0.028710\n",
      "Epoch 082 Batch 0160  Loss: 0.025209\n",
      "Epoch 082 Batch 0170  Loss: 0.032004\n",
      "Epoch 082 Batch 0180  Loss: 0.010716\n",
      "Epoch 082 Batch 0190  Loss: 0.017632\n",
      "Saved sample grid to ./samples/sample_epoch_082.png\n",
      "Epoch 083 Batch 0000  Loss: 0.046811\n",
      "Epoch 083 Batch 0010  Loss: 0.021296\n",
      "Epoch 083 Batch 0020  Loss: 0.014480\n",
      "Epoch 083 Batch 0030  Loss: 0.023421\n",
      "Epoch 083 Batch 0040  Loss: 0.034141\n",
      "Epoch 083 Batch 0050  Loss: 0.028603\n",
      "Epoch 083 Batch 0060  Loss: 0.017337\n",
      "Epoch 083 Batch 0070  Loss: 0.022304\n",
      "Epoch 083 Batch 0080  Loss: 0.028250\n",
      "Epoch 083 Batch 0090  Loss: 0.033005\n",
      "Epoch 083 Batch 0100  Loss: 0.030356\n",
      "Epoch 083 Batch 0110  Loss: 0.030997\n",
      "Epoch 083 Batch 0120  Loss: 0.014343\n",
      "Epoch 083 Batch 0130  Loss: 0.005133\n",
      "Epoch 083 Batch 0140  Loss: 0.021171\n",
      "Epoch 083 Batch 0150  Loss: 0.018313\n",
      "Epoch 083 Batch 0160  Loss: 0.040284\n",
      "Epoch 083 Batch 0170  Loss: 0.010125\n",
      "Epoch 083 Batch 0180  Loss: 0.018974\n",
      "Epoch 083 Batch 0190  Loss: 0.033885\n",
      "Saved sample grid to ./samples/sample_epoch_083.png\n",
      "Epoch 084 Batch 0000  Loss: 0.023042\n",
      "Epoch 084 Batch 0010  Loss: 0.071486\n",
      "Epoch 084 Batch 0020  Loss: 0.030310\n",
      "Epoch 084 Batch 0030  Loss: 0.009331\n",
      "Epoch 084 Batch 0040  Loss: 0.036111\n",
      "Epoch 084 Batch 0050  Loss: 0.010905\n",
      "Epoch 084 Batch 0060  Loss: 0.026277\n",
      "Epoch 084 Batch 0070  Loss: 0.038840\n",
      "Epoch 084 Batch 0080  Loss: 0.023621\n",
      "Epoch 084 Batch 0090  Loss: 0.017412\n",
      "Epoch 084 Batch 0100  Loss: 0.021529\n",
      "Epoch 084 Batch 0110  Loss: 0.033058\n",
      "Epoch 084 Batch 0120  Loss: 0.017604\n",
      "Epoch 084 Batch 0130  Loss: 0.029041\n",
      "Epoch 084 Batch 0140  Loss: 0.044345\n",
      "Epoch 084 Batch 0150  Loss: 0.010872\n",
      "Epoch 084 Batch 0160  Loss: 0.011481\n",
      "Epoch 084 Batch 0170  Loss: 0.010071\n",
      "Epoch 084 Batch 0180  Loss: 0.031938\n",
      "Epoch 084 Batch 0190  Loss: 0.036395\n",
      "Saved sample grid to ./samples/sample_epoch_084.png\n",
      "Epoch 085 Batch 0000  Loss: 0.016003\n",
      "Epoch 085 Batch 0010  Loss: 0.011394\n",
      "Epoch 085 Batch 0020  Loss: 0.047044\n",
      "Epoch 085 Batch 0030  Loss: 0.022118\n",
      "Epoch 085 Batch 0040  Loss: 0.019542\n",
      "Epoch 085 Batch 0050  Loss: 0.014358\n",
      "Epoch 085 Batch 0060  Loss: 0.027706\n",
      "Epoch 085 Batch 0070  Loss: 0.011717\n",
      "Epoch 085 Batch 0080  Loss: 0.017918\n",
      "Epoch 085 Batch 0090  Loss: 0.030311\n",
      "Epoch 085 Batch 0100  Loss: 0.006746\n",
      "Epoch 085 Batch 0110  Loss: 0.055354\n",
      "Epoch 085 Batch 0120  Loss: 0.024877\n",
      "Epoch 085 Batch 0130  Loss: 0.008937\n",
      "Epoch 085 Batch 0140  Loss: 0.014046\n",
      "Epoch 085 Batch 0150  Loss: 0.015012\n",
      "Epoch 085 Batch 0160  Loss: 0.012095\n",
      "Epoch 085 Batch 0170  Loss: 0.054368\n",
      "Epoch 085 Batch 0180  Loss: 0.015968\n",
      "Epoch 085 Batch 0190  Loss: 0.020821\n",
      "Saved sample grid to ./samples/sample_epoch_085.png\n",
      "Epoch 086 Batch 0000  Loss: 0.036400\n",
      "Epoch 086 Batch 0010  Loss: 0.036650\n",
      "Epoch 086 Batch 0020  Loss: 0.054155\n",
      "Epoch 086 Batch 0030  Loss: 0.020035\n",
      "Epoch 086 Batch 0040  Loss: 0.034300\n",
      "Epoch 086 Batch 0050  Loss: 0.026651\n",
      "Epoch 086 Batch 0060  Loss: 0.030996\n",
      "Epoch 086 Batch 0070  Loss: 0.029142\n",
      "Epoch 086 Batch 0080  Loss: 0.061506\n",
      "Epoch 086 Batch 0090  Loss: 0.019058\n",
      "Epoch 086 Batch 0100  Loss: 0.019050\n",
      "Epoch 086 Batch 0110  Loss: 0.027779\n",
      "Epoch 086 Batch 0120  Loss: 0.012865\n",
      "Epoch 086 Batch 0130  Loss: 0.040915\n",
      "Epoch 086 Batch 0140  Loss: 0.024603\n",
      "Epoch 086 Batch 0150  Loss: 0.016714\n",
      "Epoch 086 Batch 0160  Loss: 0.032269\n",
      "Epoch 086 Batch 0170  Loss: 0.021590\n",
      "Epoch 086 Batch 0180  Loss: 0.047695\n",
      "Epoch 086 Batch 0190  Loss: 0.040339\n",
      "Saved sample grid to ./samples/sample_epoch_086.png\n",
      "Epoch 087 Batch 0000  Loss: 0.032437\n",
      "Epoch 087 Batch 0010  Loss: 0.020298\n",
      "Epoch 087 Batch 0020  Loss: 0.024311\n",
      "Epoch 087 Batch 0030  Loss: 0.005149\n",
      "Epoch 087 Batch 0040  Loss: 0.046772\n",
      "Epoch 087 Batch 0050  Loss: 0.049090\n",
      "Epoch 087 Batch 0060  Loss: 0.009598\n",
      "Epoch 087 Batch 0070  Loss: 0.034661\n",
      "Epoch 087 Batch 0080  Loss: 0.047114\n",
      "Epoch 087 Batch 0090  Loss: 0.020812\n",
      "Epoch 087 Batch 0100  Loss: 0.023147\n",
      "Epoch 087 Batch 0110  Loss: 0.008887\n",
      "Epoch 087 Batch 0120  Loss: 0.040386\n",
      "Epoch 087 Batch 0130  Loss: 0.043171\n",
      "Epoch 087 Batch 0140  Loss: 0.027482\n",
      "Epoch 087 Batch 0150  Loss: 0.009269\n",
      "Epoch 087 Batch 0160  Loss: 0.047429\n",
      "Epoch 087 Batch 0170  Loss: 0.018945\n",
      "Epoch 087 Batch 0180  Loss: 0.009760\n",
      "Epoch 087 Batch 0190  Loss: 0.065339\n",
      "Saved sample grid to ./samples/sample_epoch_087.png\n",
      "Epoch 088 Batch 0000  Loss: 0.022862\n",
      "Epoch 088 Batch 0010  Loss: 0.032518\n",
      "Epoch 088 Batch 0020  Loss: 0.028834\n",
      "Epoch 088 Batch 0030  Loss: 0.023207\n",
      "Epoch 088 Batch 0040  Loss: 0.025391\n",
      "Epoch 088 Batch 0050  Loss: 0.014666\n",
      "Epoch 088 Batch 0060  Loss: 0.028761\n",
      "Epoch 088 Batch 0070  Loss: 0.015827\n",
      "Epoch 088 Batch 0080  Loss: 0.043744\n",
      "Epoch 088 Batch 0090  Loss: 0.041120\n",
      "Epoch 088 Batch 0100  Loss: 0.032382\n",
      "Epoch 088 Batch 0110  Loss: 0.029045\n",
      "Epoch 088 Batch 0120  Loss: 0.038059\n",
      "Epoch 088 Batch 0130  Loss: 0.018065\n",
      "Epoch 088 Batch 0140  Loss: 0.026139\n",
      "Epoch 088 Batch 0150  Loss: 0.038795\n",
      "Epoch 088 Batch 0160  Loss: 0.011623\n",
      "Epoch 088 Batch 0170  Loss: 0.022667\n",
      "Epoch 088 Batch 0180  Loss: 0.022734\n",
      "Epoch 088 Batch 0190  Loss: 0.017096\n",
      "Saved sample grid to ./samples/sample_epoch_088.png\n",
      "Epoch 089 Batch 0000  Loss: 0.013973\n",
      "Epoch 089 Batch 0010  Loss: 0.024189\n",
      "Epoch 089 Batch 0020  Loss: 0.025342\n",
      "Epoch 089 Batch 0030  Loss: 0.017939\n",
      "Epoch 089 Batch 0040  Loss: 0.033022\n",
      "Epoch 089 Batch 0050  Loss: 0.023429\n",
      "Epoch 089 Batch 0060  Loss: 0.026972\n",
      "Epoch 089 Batch 0070  Loss: 0.049301\n",
      "Epoch 089 Batch 0080  Loss: 0.024294\n",
      "Epoch 089 Batch 0090  Loss: 0.017081\n",
      "Epoch 089 Batch 0100  Loss: 0.017730\n",
      "Epoch 089 Batch 0110  Loss: 0.024741\n",
      "Epoch 089 Batch 0120  Loss: 0.019837\n",
      "Epoch 089 Batch 0130  Loss: 0.009513\n",
      "Epoch 089 Batch 0140  Loss: 0.011883\n",
      "Epoch 089 Batch 0150  Loss: 0.043150\n",
      "Epoch 089 Batch 0160  Loss: 0.030745\n",
      "Epoch 089 Batch 0170  Loss: 0.025085\n",
      "Epoch 089 Batch 0180  Loss: 0.019110\n",
      "Epoch 089 Batch 0190  Loss: 0.023389\n",
      "Saved sample grid to ./samples/sample_epoch_089.png\n",
      "Epoch 090 Batch 0000  Loss: 0.026717\n",
      "Epoch 090 Batch 0010  Loss: 0.017070\n",
      "Epoch 090 Batch 0020  Loss: 0.033609\n",
      "Epoch 090 Batch 0030  Loss: 0.025422\n",
      "Epoch 090 Batch 0040  Loss: 0.027395\n",
      "Epoch 090 Batch 0050  Loss: 0.009191\n",
      "Epoch 090 Batch 0060  Loss: 0.023842\n",
      "Epoch 090 Batch 0070  Loss: 0.056152\n",
      "Epoch 090 Batch 0080  Loss: 0.028154\n",
      "Epoch 090 Batch 0090  Loss: 0.020379\n",
      "Epoch 090 Batch 0100  Loss: 0.022126\n",
      "Epoch 090 Batch 0110  Loss: 0.017424\n",
      "Epoch 090 Batch 0120  Loss: 0.027721\n",
      "Epoch 090 Batch 0130  Loss: 0.018871\n",
      "Epoch 090 Batch 0140  Loss: 0.032414\n",
      "Epoch 090 Batch 0150  Loss: 0.011797\n",
      "Epoch 090 Batch 0160  Loss: 0.010088\n",
      "Epoch 090 Batch 0170  Loss: 0.035380\n",
      "Epoch 090 Batch 0180  Loss: 0.037836\n",
      "Epoch 090 Batch 0190  Loss: 0.057929\n",
      "Saved sample grid to ./samples/sample_epoch_090.png\n",
      "Epoch 091 Batch 0000  Loss: 0.025346\n",
      "Epoch 091 Batch 0010  Loss: 0.043375\n",
      "Epoch 091 Batch 0020  Loss: 0.026292\n",
      "Epoch 091 Batch 0030  Loss: 0.028913\n",
      "Epoch 091 Batch 0040  Loss: 0.025817\n",
      "Epoch 091 Batch 0050  Loss: 0.016421\n",
      "Epoch 091 Batch 0060  Loss: 0.031419\n",
      "Epoch 091 Batch 0070  Loss: 0.092112\n",
      "Epoch 091 Batch 0080  Loss: 0.048921\n",
      "Epoch 091 Batch 0090  Loss: 0.025337\n",
      "Epoch 091 Batch 0100  Loss: 0.043064\n",
      "Epoch 091 Batch 0110  Loss: 0.033796\n",
      "Epoch 091 Batch 0120  Loss: 0.026637\n",
      "Epoch 091 Batch 0130  Loss: 0.012240\n",
      "Epoch 091 Batch 0140  Loss: 0.063850\n",
      "Epoch 091 Batch 0150  Loss: 0.020269\n",
      "Epoch 091 Batch 0160  Loss: 0.033116\n",
      "Epoch 091 Batch 0170  Loss: 0.034289\n",
      "Epoch 091 Batch 0180  Loss: 0.017885\n",
      "Epoch 091 Batch 0190  Loss: 0.037675\n",
      "Saved sample grid to ./samples/sample_epoch_091.png\n",
      "Epoch 092 Batch 0000  Loss: 0.013300\n",
      "Epoch 092 Batch 0010  Loss: 0.019107\n",
      "Epoch 092 Batch 0020  Loss: 0.037126\n",
      "Epoch 092 Batch 0030  Loss: 0.012455\n",
      "Epoch 092 Batch 0040  Loss: 0.020968\n",
      "Epoch 092 Batch 0050  Loss: 0.024711\n",
      "Epoch 092 Batch 0060  Loss: 0.042250\n",
      "Epoch 092 Batch 0070  Loss: 0.028098\n",
      "Epoch 092 Batch 0080  Loss: 0.009116\n",
      "Epoch 092 Batch 0090  Loss: 0.028259\n",
      "Epoch 092 Batch 0100  Loss: 0.011954\n",
      "Epoch 092 Batch 0110  Loss: 0.026076\n",
      "Epoch 092 Batch 0120  Loss: 0.033571\n",
      "Epoch 092 Batch 0130  Loss: 0.021739\n",
      "Epoch 092 Batch 0140  Loss: 0.023449\n",
      "Epoch 092 Batch 0150  Loss: 0.026422\n",
      "Epoch 092 Batch 0160  Loss: 0.017818\n",
      "Epoch 092 Batch 0170  Loss: 0.027544\n",
      "Epoch 092 Batch 0180  Loss: 0.018649\n",
      "Epoch 092 Batch 0190  Loss: 0.031522\n",
      "Saved sample grid to ./samples/sample_epoch_092.png\n",
      "Epoch 093 Batch 0000  Loss: 0.030351\n",
      "Epoch 093 Batch 0010  Loss: 0.024198\n",
      "Epoch 093 Batch 0020  Loss: 0.035549\n",
      "Epoch 093 Batch 0030  Loss: 0.031032\n",
      "Epoch 093 Batch 0040  Loss: 0.011369\n",
      "Epoch 093 Batch 0050  Loss: 0.012202\n",
      "Epoch 093 Batch 0060  Loss: 0.020529\n",
      "Epoch 093 Batch 0070  Loss: 0.018926\n",
      "Epoch 093 Batch 0080  Loss: 0.021383\n",
      "Epoch 093 Batch 0090  Loss: 0.019243\n",
      "Epoch 093 Batch 0100  Loss: 0.026766\n",
      "Epoch 093 Batch 0110  Loss: 0.032175\n",
      "Epoch 093 Batch 0120  Loss: 0.021587\n",
      "Epoch 093 Batch 0130  Loss: 0.016311\n",
      "Epoch 093 Batch 0140  Loss: 0.045107\n",
      "Epoch 093 Batch 0150  Loss: 0.024845\n",
      "Epoch 093 Batch 0160  Loss: 0.020762\n",
      "Epoch 093 Batch 0170  Loss: 0.026291\n",
      "Epoch 093 Batch 0180  Loss: 0.024276\n",
      "Epoch 093 Batch 0190  Loss: 0.034252\n",
      "Saved sample grid to ./samples/sample_epoch_093.png\n",
      "Epoch 094 Batch 0000  Loss: 0.005596\n",
      "Epoch 094 Batch 0010  Loss: 0.007351\n",
      "Epoch 094 Batch 0020  Loss: 0.023297\n",
      "Epoch 094 Batch 0030  Loss: 0.027992\n",
      "Epoch 094 Batch 0040  Loss: 0.020298\n",
      "Epoch 094 Batch 0050  Loss: 0.024721\n",
      "Epoch 094 Batch 0060  Loss: 0.019284\n",
      "Epoch 094 Batch 0070  Loss: 0.027816\n",
      "Epoch 094 Batch 0080  Loss: 0.023532\n",
      "Epoch 094 Batch 0090  Loss: 0.019988\n",
      "Epoch 094 Batch 0100  Loss: 0.042246\n",
      "Epoch 094 Batch 0110  Loss: 0.003111\n",
      "Epoch 094 Batch 0120  Loss: 0.028155\n",
      "Epoch 094 Batch 0130  Loss: 0.024630\n",
      "Epoch 094 Batch 0140  Loss: 0.008435\n",
      "Epoch 094 Batch 0150  Loss: 0.035021\n",
      "Epoch 094 Batch 0160  Loss: 0.031405\n",
      "Epoch 094 Batch 0170  Loss: 0.028886\n",
      "Epoch 094 Batch 0180  Loss: 0.006836\n",
      "Epoch 094 Batch 0190  Loss: 0.031328\n",
      "Saved sample grid to ./samples/sample_epoch_094.png\n",
      "Epoch 095 Batch 0000  Loss: 0.029315\n",
      "Epoch 095 Batch 0010  Loss: 0.024339\n",
      "Epoch 095 Batch 0020  Loss: 0.024248\n",
      "Epoch 095 Batch 0030  Loss: 0.029906\n",
      "Epoch 095 Batch 0040  Loss: 0.013402\n",
      "Epoch 095 Batch 0050  Loss: 0.025907\n",
      "Epoch 095 Batch 0060  Loss: 0.024478\n",
      "Epoch 095 Batch 0070  Loss: 0.024226\n",
      "Epoch 095 Batch 0080  Loss: 0.024787\n",
      "Epoch 095 Batch 0090  Loss: 0.027341\n",
      "Epoch 095 Batch 0100  Loss: 0.058406\n",
      "Epoch 095 Batch 0110  Loss: 0.064971\n",
      "Epoch 095 Batch 0120  Loss: 0.022101\n",
      "Epoch 095 Batch 0130  Loss: 0.034989\n",
      "Epoch 095 Batch 0140  Loss: 0.024873\n",
      "Epoch 095 Batch 0150  Loss: 0.010214\n",
      "Epoch 095 Batch 0160  Loss: 0.013447\n",
      "Epoch 095 Batch 0170  Loss: 0.020270\n",
      "Epoch 095 Batch 0180  Loss: 0.019862\n",
      "Epoch 095 Batch 0190  Loss: 0.020079\n",
      "Saved sample grid to ./samples/sample_epoch_095.png\n",
      "Epoch 096 Batch 0000  Loss: 0.032714\n",
      "Epoch 096 Batch 0010  Loss: 0.021597\n",
      "Epoch 096 Batch 0020  Loss: 0.022618\n",
      "Epoch 096 Batch 0030  Loss: 0.029431\n",
      "Epoch 096 Batch 0040  Loss: 0.021917\n",
      "Epoch 096 Batch 0050  Loss: 0.016663\n",
      "Epoch 096 Batch 0060  Loss: 0.022591\n",
      "Epoch 096 Batch 0070  Loss: 0.014649\n",
      "Epoch 096 Batch 0080  Loss: 0.020245\n",
      "Epoch 096 Batch 0090  Loss: 0.015976\n",
      "Epoch 096 Batch 0100  Loss: 0.033969\n",
      "Epoch 096 Batch 0110  Loss: 0.022290\n",
      "Epoch 096 Batch 0120  Loss: 0.013114\n",
      "Epoch 096 Batch 0130  Loss: 0.047742\n",
      "Epoch 096 Batch 0140  Loss: 0.061563\n",
      "Epoch 096 Batch 0150  Loss: 0.022748\n",
      "Epoch 096 Batch 0160  Loss: 0.030362\n",
      "Epoch 096 Batch 0170  Loss: 0.029079\n",
      "Epoch 096 Batch 0180  Loss: 0.019884\n",
      "Epoch 096 Batch 0190  Loss: 0.014791\n",
      "Saved sample grid to ./samples/sample_epoch_096.png\n",
      "Epoch 097 Batch 0000  Loss: 0.050261\n",
      "Epoch 097 Batch 0010  Loss: 0.015705\n",
      "Epoch 097 Batch 0020  Loss: 0.067700\n",
      "Epoch 097 Batch 0030  Loss: 0.013238\n",
      "Epoch 097 Batch 0040  Loss: 0.042722\n",
      "Epoch 097 Batch 0050  Loss: 0.034476\n",
      "Epoch 097 Batch 0060  Loss: 0.026646\n",
      "Epoch 097 Batch 0070  Loss: 0.009135\n",
      "Epoch 097 Batch 0080  Loss: 0.022822\n",
      "Epoch 097 Batch 0090  Loss: 0.024449\n",
      "Epoch 097 Batch 0100  Loss: 0.011449\n",
      "Epoch 097 Batch 0110  Loss: 0.030394\n",
      "Epoch 097 Batch 0120  Loss: 0.023365\n",
      "Epoch 097 Batch 0130  Loss: 0.021240\n",
      "Epoch 097 Batch 0140  Loss: 0.018416\n",
      "Epoch 097 Batch 0150  Loss: 0.013261\n",
      "Epoch 097 Batch 0160  Loss: 0.063957\n",
      "Epoch 097 Batch 0170  Loss: 0.037092\n",
      "Epoch 097 Batch 0180  Loss: 0.021235\n",
      "Epoch 097 Batch 0190  Loss: 0.046876\n",
      "Saved sample grid to ./samples/sample_epoch_097.png\n",
      "Epoch 098 Batch 0000  Loss: 0.039409\n",
      "Epoch 098 Batch 0010  Loss: 0.033808\n",
      "Epoch 098 Batch 0020  Loss: 0.021705\n",
      "Epoch 098 Batch 0030  Loss: 0.040667\n",
      "Epoch 098 Batch 0040  Loss: 0.047686\n",
      "Epoch 098 Batch 0050  Loss: 0.024054\n",
      "Epoch 098 Batch 0060  Loss: 0.011554\n",
      "Epoch 098 Batch 0070  Loss: 0.013705\n",
      "Epoch 098 Batch 0080  Loss: 0.042594\n",
      "Epoch 098 Batch 0090  Loss: 0.018938\n",
      "Epoch 098 Batch 0100  Loss: 0.038722\n",
      "Epoch 098 Batch 0110  Loss: 0.037824\n",
      "Epoch 098 Batch 0120  Loss: 0.009547\n",
      "Epoch 098 Batch 0130  Loss: 0.027776\n",
      "Epoch 098 Batch 0140  Loss: 0.018333\n",
      "Epoch 098 Batch 0150  Loss: 0.028936\n",
      "Epoch 098 Batch 0160  Loss: 0.009400\n",
      "Epoch 098 Batch 0170  Loss: 0.029111\n",
      "Epoch 098 Batch 0180  Loss: 0.021974\n",
      "Epoch 098 Batch 0190  Loss: 0.020602\n",
      "Saved sample grid to ./samples/sample_epoch_098.png\n",
      "Epoch 099 Batch 0000  Loss: 0.017467\n",
      "Epoch 099 Batch 0010  Loss: 0.024819\n",
      "Epoch 099 Batch 0020  Loss: 0.010715\n",
      "Epoch 099 Batch 0030  Loss: 0.013391\n",
      "Epoch 099 Batch 0040  Loss: 0.026641\n",
      "Epoch 099 Batch 0050  Loss: 0.038435\n",
      "Epoch 099 Batch 0060  Loss: 0.012867\n",
      "Epoch 099 Batch 0070  Loss: 0.064473\n",
      "Epoch 099 Batch 0080  Loss: 0.015845\n",
      "Epoch 099 Batch 0090  Loss: 0.023912\n",
      "Epoch 099 Batch 0100  Loss: 0.038732\n",
      "Epoch 099 Batch 0110  Loss: 0.020584\n",
      "Epoch 099 Batch 0120  Loss: 0.013150\n",
      "Epoch 099 Batch 0130  Loss: 0.023751\n",
      "Epoch 099 Batch 0140  Loss: 0.021403\n",
      "Epoch 099 Batch 0150  Loss: 0.025537\n",
      "Epoch 099 Batch 0160  Loss: 0.037633\n",
      "Epoch 099 Batch 0170  Loss: 0.016631\n",
      "Epoch 099 Batch 0180  Loss: 0.021170\n",
      "Epoch 099 Batch 0190  Loss: 0.018138\n",
      "Saved sample grid to ./samples/sample_epoch_099.png\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.op = nn.Conv2d(ch, ch, kernel_size=4, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch=3, base_ch=64, time_emb_dim=128, channel_mults=(1,2,4)):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinCosPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        self.init_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        chs = [base_ch * m for m in channel_mults]\n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        in_c = base_ch\n",
    "        for out_c in chs:\n",
    "            self.enc_blocks.append(ResidualBlock(in_c, out_c, time_emb_dim=time_emb_dim))\n",
    "            self.downs.append(Downsample(out_c))\n",
    "            in_c = out_c\n",
    "        self.mid1 = ResidualBlock(in_c, in_c*2, time_emb_dim=time_emb_dim)\n",
    "        self.mid2 = ResidualBlock(in_c*2, in_c, time_emb_dim=time_emb_dim)\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        for out_c in reversed(chs):\n",
    "            self.ups.append(Upsample(in_c))\n",
    "            self.dec_blocks.append(ResidualBlock(in_c + out_c, out_c, time_emb_dim=time_emb_dim))\n",
    "            in_c = out_c\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8 if in_c>=8 else 1, num_channels=in_c),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_c, in_ch, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        h = self.init_conv(x)\n",
    "        skips = []\n",
    "        for enc, down in zip(self.enc_blocks, self.downs):\n",
    "            h = enc(h, t_emb)\n",
    "            skips.append(h)\n",
    "            h = down(h)\n",
    "        h = self.mid1(h, t_emb)\n",
    "        h = self.mid2(h, t_emb)\n",
    "        for up, dec, skip in zip(self.ups, self.dec_blocks, reversed(skips)):\n",
    "            h = up(h)\n",
    "            if skip.shape[-2:] != h.shape[-2:]:\n",
    "                _,_,H,W = h.shape\n",
    "                skip = skip[..., :H, :W]\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            h = dec(h, t_emb)\n",
    "        out = self.final_conv(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = UNetSmall(in_ch=3, base_ch=64, time_emb_dim=128).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "def forward_process(x0, t, alpha_bar):\n",
    "\n",
    "    noise = torch.randn_like(x0)\n",
    "    sqrt_ab = torch.sqrt(alpha_bar[t]).view(-1,1,1,1)\n",
    "    sqrt_1m = torch.sqrt(1.0 - alpha_bar[t]).view(-1,1,1,1)\n",
    "    xt = sqrt_ab * x0 + sqrt_1m * noise\n",
    "    return xt, noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, shape, betas, alphas, alpha_bar, alpha_bar_prev, device):\n",
    "    model.eval()\n",
    "    x = torch.randn(shape, device=device)  \n",
    "\n",
    "    for step in reversed(range(DIFFUSION_STEPS)):\n",
    "        t = step\n",
    "        ts = torch.full((shape[0],), t, device=device, dtype=torch.long)\n",
    "        eps_pred = model(x, ts)\n",
    "\n",
    "        a_t = alphas[t]               \n",
    "        ab_t = alpha_bar[t]\n",
    "        ab_prev = alpha_bar_prev[t]\n",
    "        b_t = betas[t]\n",
    "\n",
    "        x0_pred = (x - torch.sqrt(1.0 - ab_t) * eps_pred) / (torch.sqrt(ab_t) + 1e-8)\n",
    "\n",
    "        coef1 = torch.sqrt(ab_prev) * b_t / (1.0 - ab_t)\n",
    "        coef2 = torch.sqrt(a_t) * (1.0 - ab_prev) / (1.0 - ab_t)\n",
    "        mean = coef1 * x0_pred + coef2 * x\n",
    "\n",
    "        if step > 0:\n",
    "            x = mean + torch.sqrt(b_t) * torch.randn_like(x)\n",
    "        else:\n",
    "            x = mean\n",
    "\n",
    "    return x.clamp(-1, 1)\n",
    "\n",
    "alpha_bar = cum_alpha_bar.to(DEVICE)\n",
    "alpha_bar_prev = alpha_bar_prev.to(DEVICE)\n",
    "\n",
    "print(\"Starting training on this device\", DEVICE)\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, x0 in enumerate(loader):\n",
    "        if batch_idx >= MAX_BATCHES:\n",
    "            break\n",
    "\n",
    "        x0 = x0.to(DEVICE)\n",
    "        t = torch.randint(0, DIFFUSION_STEPS, (x0.size(0),), device=DEVICE, dtype=torch.long)\n",
    "\n",
    "        noise = torch.randn_like(x0)\n",
    "        sqrt_ab = torch.sqrt(alpha_bar[t]).view(-1, 1, 1, 1)\n",
    "        sqrt_1m = torch.sqrt(1.0 - alpha_bar[t]).view(-1, 1, 1, 1)\n",
    "        xt = sqrt_ab * x0 + sqrt_1m * noise\n",
    "\n",
    "        noise_pred = model(xt, t)\n",
    "\n",
    "        loss = mse(noise_pred, noise)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d} Batch {batch_idx:04d}  Loss: {loss.item():.6f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = sample(\n",
    "            model,\n",
    "            shape=(SAMPLE_BATCH, 3, IMG_SIZE, IMG_SIZE),\n",
    "            betas=betas,\n",
    "            alphas=alphas,\n",
    "            alpha_bar=alpha_bar,\n",
    "            alpha_bar_prev=alpha_bar_prev,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "    samples_01 = (samples * 0.5) + 0.5\n",
    "    grid = vutils.make_grid(samples_01, nrow=4, padding=2, normalize=False)\n",
    "    out_path = os.path.join(OUT_DIR, f'sample_epoch_{epoch:03d}.png')\n",
    "    vutils.save_image(grid, out_path)\n",
    "    print(f\"Saved sample grid to {out_path}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
